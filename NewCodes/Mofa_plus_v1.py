# -*- coding: utf-8 -*-
"""Mofa_Manifold_Diffusion_Final.ipynb

Automatically generated by Colab.
"""

# ===============================================
# 1. IMPORTS & SETUP
# ===============================================
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import os

from mofapy2.run.entry_point import entry_point
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (accuracy_score, f1_score, recall_score, 
                             classification_report, confusion_matrix, roc_auc_score)
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder, label_binarize
from scipy.linalg import inv
from sklearn.manifold import TSNE

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Create output folder
output_folder = "results_mofa_prototype"
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

# ===============================================
# 2. DATA LOADING & MOFA+ INTEGRATION
# ===============================================
print("Loading pre-processed datasets...")

# Load your datasets (Ensure paths are correct)
try:
    expression_data_scaled = pd.read_csv("NewDatasets/expression_data_scaled_FXS_MOFA_3Omics.csv", index_col=0)
    methylation_scaled = pd.read_csv("NewDatasets/methylation_scaled_FXS_MOFA_3Omics.csv", index_col=0)
    copy_number_scaled = pd.read_csv("NewDatasets/copy_number_scaled_FXS_MOFA_3Omics.csv", index_col=0)
    subtype_encoded = pd.read_csv("NewDatasets/subtype_encoded_FXS_MOFA_3Omics.csv", index_col=0).squeeze()
    phenotype_data_clean = pd.read_csv("NewDatasets/phenotype_data_clean_FXS_MOFA_3Omics.csv", index_col=0)
    
    common_samples = list(expression_data_scaled.columns)
    print(f"‚úÖ Data loaded. Common samples: {len(common_samples)}")

    # Prepare Data for MOFA
    data_list = [
        [expression_data_scaled.T.values],   # view 0
        [methylation_scaled.T.values],       # view 1
        [copy_number_scaled.T.values]        # view 2
    ]

    # Initialize MOFA
    ent = entry_point()
    ent.set_data_options(scale_views=True, scale_groups=False)
    ent.set_data_matrix(data_list)
    ent.set_model_options(factors=15, spikeslab_factors=True, ard_factors=True)
    ent.set_train_options(iter=400, convergence_mode="slow", seed=42, verbose=False, gpu_mode=True)

    print("Running MOFA+...")
    ent.build()
    ent.run()

    # Extract Latent Factors (Z)
    factors = ent.model.nodes["Z"].getExpectation()
    print(f"Factors shape: {factors.shape}")

except Exception as e:
    print(f"‚ö†Ô∏è Error in Data Loading/MOFA: {e}")
    # Create dummy data for testing the pipeline if files are missing
    print("Creating DUMMY data for pipeline verification...")
    factors = np.random.rand(260, 15)
    subtype_encoded = pd.Series(np.random.choice(['LMS', 'DDLPS', 'UPS', 'MFS'], size=260))

# ===============================================
# 3. ADAPTIVE GRAPH CONSTRUCTION (Refinement #1)
# ===============================================
def get_adaptive_adjacency(latent_data, k=10, metric='euclidean'):
    """
    Constructs an adjacency matrix using Zelnik-Manor local scaling.
    Adapts sigma per patient based on local density.
    """
    n_samples = latent_data.shape[0]
    
    # 1. Compute KNN (k+1 to include self)
    metric_knn = 'cosine' if metric == 'cosine' else 'euclidean'
    nbrs = NearestNeighbors(n_neighbors=k+1, metric=metric_knn).fit(latent_data)
    dists, indices = nbrs.kneighbors(latent_data)
    
    A = np.zeros((n_samples, n_samples))
    
    # 2. Adaptive Sigma: Distance to the k-th neighbor
    # We use min(k, 7) to avoid noise if k is large
    k_scale = min(k, 7)
    sigma = dists[:, k_scale]  
    sigma[sigma == 0] = np.mean(sigma) + 1e-10 # Safety

    # 3. Compute Similarity
    for i in range(n_samples):
        for j_idx in range(1, k + 1): # Skip self
            neighbor = indices[i, j_idx]
            d = dists[i, j_idx]
            
            if metric_knn == 'euclidean':
                # Zelnik-Manor scaling: sigma_i * sigma_j
                denom = sigma[i] * sigma[neighbor]
                sim_score = np.exp(- (d**2) / denom)
            else:
                sim_score = 1 - d
                if sim_score < 0: sim_score = 0
            
            A[i, neighbor] = sim_score
            
    # Symmetrize
    A = np.maximum(A, A.T)
    return A

# ===============================================
# 4. PIPELINE EVALUATION (Fix #1 & Refinement #2)
# ===============================================
def evaluate_pipeline(Z, y_true, k, alpha, dist_metric='euclidean'):
    """
    Runs 5-fold CV with Rare Class Normalization and Medoid Prototypes.
    """
    try:
        # 1. Construct Adaptive Graph (Transductive Setting)
        A_cv = get_adaptive_adjacency(Z, k=k, metric=dist_metric)
        
        # Random Walk Matrix
        deg = np.sum(A_cv, axis=1)
        deg[deg==0] = 1e-10
        D_inv = np.diag(1/deg)
        W_cv = D_inv @ A_cv
        
        # Diffusion Operator: (I - alpha*W)^-1
        I = np.eye(Z.shape[0])
        Diffusion_Op = inv(I - alpha * W_cv)
        
    except Exception as e:
        print(f"Graph failure: {e}")
        return 0, 0, 0
        
    # 2. Cross Validation
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    acc_scores, f1_scores, recall_scores = [], [], []
    
    unique_classes_cv = np.unique(y_true)
    y_true_arr = np.array(y_true)
    
    for train_idx, test_idx in skf.split(Z, y_true):
        # --- CRITICAL FIX #1: Signal Normalization ---
        # Inject signal only for Training nodes, normalized by class size.
        n_samples = Z.shape[0]
        signal_cv = np.zeros((n_samples, len(unique_classes_cv)))
        
        for i, cls in enumerate(unique_classes_cv):
            cls_mask = (y_true_arr[train_idx] == cls)
            global_train_idxs = train_idx[cls_mask]
            
            # NORMALIZATION: Total mass = 1.0 regardless of class size
            n_cls = len(global_train_idxs)
            if n_cls > 0:
                signal_cv[global_train_idxs, i] = 1.0 / n_cls
        
        # Propagate Signal
        diffused_signal_cv = Diffusion_Op @ signal_cv
        
        # --- REFINEMENT 2: Medoid Prototypes ---
        medoid_prototypes = {}
        for i, cls in enumerate(unique_classes_cv):
            # Select TRAINING patient with highest diffused weight for this class
            train_weights = diffused_signal_cv[train_idx, i]
            best_train_idx_local = np.argmax(train_weights)
            best_train_idx_global = train_idx[best_train_idx_local]
            medoid_prototypes[cls] = Z[best_train_idx_global]
            
        # Predict Test Nodes
        distances_test = np.zeros((len(test_idx), len(unique_classes_cv)))
        Z_test = Z[test_idx]
        
        for i, cls in enumerate(unique_classes_cv):
            proto = medoid_prototypes[cls]
            distances_test[:, i] = np.linalg.norm(Z_test - proto, axis=1)
            
        # --- REFINEMENT 3: Class Prior Weighting ---
        train_labels = y_true_arr[train_idx]
        _, counts = np.unique(train_labels, return_counts=True)
        # Inverse probability weighting
        class_weights = np.sum(counts) / (counts * len(unique_classes_cv))
        
        # Score = exp(-dist) * class_weight
        scores = np.exp(-distances_test) * class_weights
        
        preds_indices = np.argmax(scores, axis=1)
        preds = unique_classes_cv[preds_indices]
        
        # Score
        y_test = y_true_arr[test_idx]
        acc_scores.append(accuracy_score(y_test, preds))
        f1_scores.append(f1_score(y_test, preds, average='macro'))
        recall_scores.append(recall_score(y_test, preds, average='macro'))
        
    return np.mean(acc_scores), np.mean(f1_scores), np.mean(recall_scores)

# ===============================================
# 5. GRID SEARCH & FINAL EVALUATION
# ===============================================
print("\n" + "="*50)
print("HYPERPARAMETER TUNING")
print("="*50)

# Settings
y_data = subtype_encoded.values
Z_data = factors
k_values = [5, 10, 15]
alpha_values = [0.5, 0.8, 0.9]
metrics = ['euclidean']

best_score = -1
best_params = {}

print(f"{'Metric':<10} | {'k':<5} | {'Alpha':<5} | {'Acc':<8} | {'Mac-F1':<8} | {'Recall':<8}")
print("-" * 65)

for metric in metrics:
    for k in k_values:
        for alpha in alpha_values:
            acc, f1, rec = evaluate_pipeline(Z_data, y_data, k, alpha, metric)
            print(f"{metric:<10} | {k:<5} | {alpha:<5} | {acc:.4f}   | {f1:.4f}   | {rec:.4f}")
            
            if f1 > best_score:
                best_score = f1
                best_params = {'metric': metric, 'k': k, 'alpha': alpha}

print("-" * 65)
print(f"üèÜ Best Parameters (Macro F1): {best_params}")

# ===============================================
# 6. FINAL MODEL REPORT
# ===============================================
print("\n" + "="*50)
print("FINAL EVALUATION (BEST MODEL)")
print("="*50)

k_best = best_params.get('k', 10)
alpha_best = best_params.get('alpha', 0.8)
metric_best = best_params.get('metric', 'euclidean')

# Stratified Split for Report
unique_classes_full = np.unique(y_data)
train_idx, test_idx = train_test_split(np.arange(len(y_data)), test_size=0.2, stratify=y_data, random_state=42)

# Re-Build Graph
A_final = get_adaptive_adjacency(Z_data, k=k_best, metric=metric_best)
deg_final = np.sum(A_final, axis=1)
deg_final[deg_final==0] = 1e-10
D_inv_final = np.diag(1/deg_final)
W_final = D_inv_final @ A_final
Diff_Op_Final = inv(np.eye(Z_data.shape[0]) - alpha_best * W_final)

# Signal Injection (Normalized)
signal_final = np.zeros((Z_data.shape[0], len(unique_classes_full)))
for i, cls in enumerate(unique_classes_full):
    cls_mask = (y_data[train_idx] == cls)
    global_train_idxs = train_idx[cls_mask]
    if len(global_train_idxs) > 0:
        signal_final[global_train_idxs, i] = 1.0 / len(global_train_idxs)

# Diffuse & Medoids
diffused_signals_final = Diff_Op_Final @ signal_final
final_prototypes = {}
for i, cls in enumerate(unique_classes_full):
    train_weights = diffused_signals_final[train_idx, i]
    best_idx = train_idx[np.argmax(train_weights)]
    final_prototypes[cls] = Z_data[best_idx]

# Predict
Z_test = Z_data[test_idx]
dists_test = np.zeros((len(test_idx), len(unique_classes_full)))
for i, cls in enumerate(unique_classes_full):
    dists_test[:, i] = np.linalg.norm(Z_test - final_prototypes[cls], axis=1)

# Prior Weights
_, counts_train = np.unique(y_data[train_idx], return_counts=True)
prior_weights = np.sum(counts_train) / (counts_train * len(unique_classes_full))

scores = np.exp(-dists_test) * prior_weights
preds_final = unique_classes_full[np.argmax(scores, axis=1)]
y_test = y_data[test_idx]

# Reports
print("\nClassification Report:")
print(classification_report(y_test, preds_final, zero_division=0))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, preds_final, labels=unique_classes_full)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes_full, yticklabels=unique_classes_full)
plt.title('Confusion Matrix')
plt.ylabel('True')
plt.xlabel('Predicted')
plt.show()

# Rare Class Analysis
print("\nRare Class Analysis:")
counts = pd.Series(y_data).value_counts()
rare_classes = counts[counts < 15].index.tolist()
for r_cls in rare_classes:
    r_indices = np.where(y_test == r_cls)[0]
    if len(r_indices) > 0:
        r_correct = np.sum(preds_final[r_indices] == r_cls)
        print(f"  Class '{r_cls}': {r_correct}/{len(r_indices)} correct")
    else:
        print(f"  Class '{r_cls}': No test samples.")