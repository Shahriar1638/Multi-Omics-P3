{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46bd621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running on: cuda\n",
      ">>> Generating 250 samples (High Dim -> Filtered)...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1. INSTALL OPTUNA IF MISSING\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    print(\"Installing optuna...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION & DEVICE\n",
    "# ==========================================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\">>> Running on: {DEVICE}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. ROBUST DATA GENERATOR (Variance Filtered)\n",
    "# ==========================================\n",
    "def generate_optimized_data(n_samples=250):\n",
    "    print(f\">>> Generating {n_samples} samples (High Dim -> Filtered)...\")\n",
    "    y = np.random.randint(0, 3, n_samples)\n",
    "\n",
    "    # RNA (30k features, sparse signal)\n",
    "    rna = np.random.randn(n_samples, 30000) * 5\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 0: rna[i, 0:100] += 12.0\n",
    "        if y[i] == 1: rna[i, 100:200] += 12.0\n",
    "\n",
    "    # Meth (30k features)\n",
    "    meth = np.random.rand(n_samples, 30600)\n",
    "    for i in range(n_samples):\n",
    "        if y[i] == 2: meth[i, 0:50] = np.clip(meth[i, 0:50] + 0.8, 0, 1)\n",
    "\n",
    "    # Clin (60 features)\n",
    "    clin = np.random.randn(n_samples, 60)\n",
    "\n",
    "    # --- VARIANCE FILTERING (The \"Needle Finder\") ---\n",
    "    # We filter BEFORE scaling to keep the signal peaks\n",
    "    def filter_top_k(data, k):\n",
    "        vars = np.var(data, axis=0)\n",
    "        idx = np.argsort(vars)[-k:] # Indices of highest variance\n",
    "        return data[:, np.sort(idx)]\n",
    "\n",
    "    # Keep top 3000 RNA and 2000 Meth features\n",
    "    rna = filter_top_k(rna, 3000)\n",
    "    meth = filter_top_k(meth, 2000)\n",
    "\n",
    "    # Scale\n",
    "    rna = StandardScaler().fit_transform(rna)\n",
    "    meth = StandardScaler().fit_transform(meth)\n",
    "    clin = StandardScaler().fit_transform(clin)\n",
    "\n",
    "    # Return as numpy arrays (we will convert to Tensor inside CV loop)\n",
    "    dims = (rna.shape[1], meth.shape[1], clin.shape[1])\n",
    "    return rna, meth, clin, y, dims\n",
    "\n",
    "# Generate Data Once\n",
    "X_rna, X_meth, X_clin, Y_labels, DIMS = generate_optimized_data(250)\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL CLASSES\n",
    "# ==========================================\n",
    "# ==========================================\n",
    "# 2. MODEL CLASSES (FIXED)\n",
    "# ==========================================\n",
    "class PerOmicCMAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LayerNorm(512), nn.GELU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512), nn.GELU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim), nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask_ratio=0.0):\n",
    "        if mask_ratio > 0 and self.training:\n",
    "            mask = (torch.rand_like(x) > mask_ratio).float()\n",
    "            x_masked = x * mask\n",
    "        else:\n",
    "            mask = torch.ones_like(x)\n",
    "            x_masked = x\n",
    "        z = self.encoder(x_masked)\n",
    "\n",
    "        # --- FIX: RETURN 4 VALUES (Added 'mask') ---\n",
    "        return self.decoder(z), self.projector(z), z, mask\n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(self, latent_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.gate_rna = nn.Linear(latent_dim, 1)\n",
    "        self.gate_meth = nn.Linear(latent_dim, 1)\n",
    "        self.gate_clin = nn.Linear(latent_dim, 1)\n",
    "        self.classifier = nn.Linear(latent_dim, 3)\n",
    "        self.drop_rate = dropout_rate\n",
    "\n",
    "    def forward(self, z_rna, z_meth, z_clin, apply_dropout=False):\n",
    "        if apply_dropout and self.training:\n",
    "            if torch.rand(1).item() < self.drop_rate: z_rna = torch.zeros_like(z_rna)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_meth = torch.zeros_like(z_meth)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_clin = torch.zeros_like(z_clin)\n",
    "\n",
    "        w_rna = torch.sigmoid(self.gate_rna(z_rna))\n",
    "        w_meth = torch.sigmoid(self.gate_meth(z_meth))\n",
    "        w_clin = torch.sigmoid(self.gate_clin(z_clin))\n",
    "\n",
    "        z_fused = (w_rna * z_rna + w_meth * z_meth + w_clin * z_clin) / (w_rna + w_meth + w_clin + 1e-8)\n",
    "        return self.classifier(z_fused), torch.cat([w_rna, w_meth, w_clin], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de12e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StabilizedUncertaintyLoss(nn.Module):\n",
    "    def __init__(self, num_losses):\n",
    "        super().__init__()\n",
    "        self.log_vars = nn.Parameter(torch.zeros(num_losses))\n",
    "    def forward(self, losses):\n",
    "        total = 0\n",
    "        for i, loss in enumerate(losses):\n",
    "            prec = torch.clamp(0.5 * torch.exp(-self.log_vars[i]), 0.2, 3.0)\n",
    "            total += prec * loss + 0.5 * self.log_vars[i]\n",
    "        return total\n",
    "\n",
    "def contrastive_loss(q, k, queue, temp=0.1):\n",
    "    q = F.normalize(q, dim=1); k = F.normalize(k, dim=1); queue = queue.detach()\n",
    "    l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "    l_neg = torch.einsum('nc,ck->nk', [q, queue])\n",
    "    logits = torch.cat([l_pos, l_neg], dim=1) / temp\n",
    "    return F.cross_entropy(logits, torch.zeros(logits.shape[0], dtype=torch.long).to(q.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a6005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. OPTUNA OBJECTIVE FUNCTION (FIXED)\n",
    "# ==========================================\n",
    "def objective(trial):\n",
    "    # --- A. Suggest Hyperparameters ---\n",
    "    lr_pre = trial.suggest_float(\"lr_pre\", 1e-4, 5e-3, log=True)\n",
    "    mask_ratio = trial.suggest_float(\"mask_ratio\", 0.25, 0.75)\n",
    "    latent_dim = trial.suggest_categorical(\"latent_dim\", [64, 128])\n",
    "\n",
    "    lr_fine = trial.suggest_float(\"lr_fine\", 1e-4, 1e-2, log=True)\n",
    "    lambda_ent = trial.suggest_float(\"lambda_ent\", 0.0, 0.2)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4)\n",
    "\n",
    "    # --- B. Cross-Validation Loop ---\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_rna, Y_labels)):\n",
    "\n",
    "        # 1. Prepare Data\n",
    "        tr_ds = TensorDataset(\n",
    "            torch.FloatTensor(X_rna[train_idx]), torch.FloatTensor(X_meth[train_idx]),\n",
    "            torch.FloatTensor(X_clin[train_idx]), torch.LongTensor(Y_labels[train_idx])\n",
    "        )\n",
    "        val_ds = TensorDataset(\n",
    "            torch.FloatTensor(X_rna[val_idx]), torch.FloatTensor(X_meth[val_idx]),\n",
    "            torch.FloatTensor(X_clin[val_idx]), torch.LongTensor(Y_labels[val_idx])\n",
    "        )\n",
    "\n",
    "        tr_loader = DataLoader(tr_ds, batch_size=32, shuffle=True, drop_last=True)\n",
    "        val_rna, val_meth, val_clin, val_y = val_ds[:]\n",
    "        val_rna, val_meth, val_clin, val_y = val_rna.to(DEVICE), val_meth.to(DEVICE), val_clin.to(DEVICE), val_y.to(DEVICE)\n",
    "\n",
    "        # 2. Init Models\n",
    "        cmae_r = PerOmicCMAE(DIMS[0], latent_dim).to(DEVICE)\n",
    "        cmae_m = PerOmicCMAE(DIMS[1], latent_dim).to(DEVICE)\n",
    "        cmae_c = PerOmicCMAE(DIMS[2], latent_dim).to(DEVICE)\n",
    "        mem_bank = nn.Parameter(F.normalize(torch.randn(latent_dim, 128), dim=0), requires_grad=False).to(DEVICE)\n",
    "        loss_fn = StabilizedUncertaintyLoss(4).to(DEVICE)\n",
    "\n",
    "        opt_pre = optim.AdamW(list(cmae_r.parameters())+list(cmae_m.parameters())+list(cmae_c.parameters())+list(loss_fn.parameters()), lr=lr_pre)\n",
    "\n",
    "        # 3. Phase 1: Pre-training\n",
    "        cmae_r.train(); cmae_m.train(); cmae_c.train()\n",
    "        for epoch in range(15): # Reduced epochs for speed\n",
    "            for r, m, c, _ in tr_loader:\n",
    "                r, m, c = r.to(DEVICE), m.to(DEVICE), c.to(DEVICE)\n",
    "\n",
    "                # Forward View 1 (Now unpacking 4 values works)\n",
    "                rec_r1, proj_r1, _, _ = cmae_r(r, mask_ratio)\n",
    "                rec_m1, proj_m1, _, _ = cmae_m(m, mask_ratio)\n",
    "                rec_c1, proj_c1, _, _ = cmae_c(c, mask_ratio)\n",
    "\n",
    "                # Forward View 2\n",
    "                with torch.no_grad():\n",
    "                    _, proj_r2, _, _ = cmae_r(r, mask_ratio)\n",
    "                    _, proj_m2, _, _ = cmae_m(m, mask_ratio)\n",
    "                    _, proj_c2, _, _ = cmae_c(c, mask_ratio)\n",
    "\n",
    "                loss = loss_fn([\n",
    "                    F.mse_loss(rec_r1, r), F.mse_loss(rec_m1, m), F.mse_loss(rec_c1, c),\n",
    "                    (contrastive_loss(proj_r1, proj_r2, mem_bank) +\n",
    "                     contrastive_loss(proj_m1, proj_m2, mem_bank) +\n",
    "                     contrastive_loss(proj_c1, proj_c2, mem_bank))/3\n",
    "                ])\n",
    "\n",
    "                opt_pre.zero_grad(); loss.backward(); opt_pre.step()\n",
    "                with torch.no_grad():\n",
    "                    avg_proj = (proj_r1 + proj_m1 + proj_c1) / 3\n",
    "                    mem_bank.data = torch.cat([mem_bank[:, avg_proj.shape[0]:], avg_proj.T], dim=1)\n",
    "\n",
    "        # 4. Phase 2: Fine-tuning\n",
    "        cmae_r.eval(); cmae_m.eval(); cmae_c.eval()\n",
    "        fusion = GatedAttentionFusion(latent_dim, dropout_rate).to(DEVICE)\n",
    "        opt_fine = optim.AdamW(fusion.parameters(), lr=lr_fine)\n",
    "\n",
    "        best_fold_acc = 0\n",
    "\n",
    "        for epoch in range(20):\n",
    "            fusion.train()\n",
    "            for r, m, c, y in tr_loader:\n",
    "                r, m, c, y = r.to(DEVICE), m.to(DEVICE), c.to(DEVICE), y.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    # --- FIX: UNPACK 4 VALUES HERE TOO (using _) ---\n",
    "                    _, _, zr, _ = cmae_r(r)\n",
    "                    _, _, zm, _ = cmae_m(m)\n",
    "                    _, _, zc, _ = cmae_c(c)\n",
    "\n",
    "                logits, weights = fusion(zr, zm, zc, apply_dropout=True)\n",
    "                cls_loss = F.cross_entropy(logits, y)\n",
    "                entropy = -torch.sum(weights * torch.log(weights + 1e-8), dim=1).mean()\n",
    "                loss = cls_loss + lambda_ent * entropy\n",
    "\n",
    "                opt_fine.zero_grad(); loss.backward(); opt_fine.step()\n",
    "\n",
    "            fusion.eval()\n",
    "            with torch.no_grad():\n",
    "                # --- FIX: UNPACK 4 VALUES HERE TOO ---\n",
    "                _, _, zr, _ = cmae_r(val_rna)\n",
    "                _, _, zm, _ = cmae_m(val_meth)\n",
    "                _, _, zc, _ = cmae_c(val_clin)\n",
    "\n",
    "                logits, _ = fusion(zr, zm, zc, apply_dropout=False)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                acc = accuracy_score(val_y.cpu(), preds.cpu())\n",
    "                if acc > best_fold_acc: best_fold_acc = acc\n",
    "\n",
    "        fold_accuracies.append(best_fold_acc)\n",
    "\n",
    "        trial.report(np.mean(fold_accuracies), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0b2e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 17:52:22,047] A new study created in memory with name: no-name-41a40957-a238-4dde-850f-d1222eb8fefc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> STARTING OPTUNA OPTIMIZATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 17:52:35,181] Trial 0 finished with value: 0.8515490533562824 and parameters: {'lr_pre': 0.0035423823955040546, 'mask_ratio': 0.6109969151330155, 'latent_dim': 128, 'lr_fine': 0.00037231126343362887, 'lambda_ent': 0.16470167790199763, 'dropout_rate': 0.25398624104882483}. Best is trial 0 with value: 0.8515490533562824.\n",
      "[I 2025-12-05 17:52:38,019] Trial 1 finished with value: 1.0 and parameters: {'lr_pre': 0.0005317396233045405, 'mask_ratio': 0.6240695825197669, 'latent_dim': 128, 'lr_fine': 0.009936337739842087, 'lambda_ent': 0.11151781859880357, 'dropout_rate': 0.1987559624907232}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:52:40,837] Trial 2 finished with value: 1.0 and parameters: {'lr_pre': 0.00020587200642208797, 'mask_ratio': 0.5081624221478677, 'latent_dim': 128, 'lr_fine': 0.0036192923771001034, 'lambda_ent': 0.07990136521314811, 'dropout_rate': 0.28303524107474054}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:52:43,700] Trial 3 finished with value: 0.9839357429718875 and parameters: {'lr_pre': 0.0009413026704375432, 'mask_ratio': 0.5947743445863483, 'latent_dim': 128, 'lr_fine': 0.0021524935719593272, 'lambda_ent': 0.1128817818311382, 'dropout_rate': 0.1909032575109776}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:52:46,593] Trial 4 finished with value: 0.8719640466628418 and parameters: {'lr_pre': 0.00025155421348974896, 'mask_ratio': 0.7461818050581671, 'latent_dim': 64, 'lr_fine': 0.000518217773113746, 'lambda_ent': 0.0516952535998259, 'dropout_rate': 0.24649626491012963}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:52:49,427] Trial 5 finished with value: 1.0 and parameters: {'lr_pre': 0.0005656559488101003, 'mask_ratio': 0.5774113381640199, 'latent_dim': 128, 'lr_fine': 0.0007824201085997815, 'lambda_ent': 0.004849139555349225, 'dropout_rate': 0.13258265623990434}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:52:50,361] Trial 6 pruned. \n",
      "[I 2025-12-05 17:52:51,335] Trial 7 pruned. \n",
      "[I 2025-12-05 17:52:52,311] Trial 8 pruned. \n",
      "[I 2025-12-05 17:52:55,144] Trial 9 finished with value: 1.0 and parameters: {'lr_pre': 0.000441178706701523, 'mask_ratio': 0.2563515713709176, 'latent_dim': 128, 'lr_fine': 0.0036238960778674062, 'lambda_ent': 0.005686003934120687, 'dropout_rate': 0.3415284917103856}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:52:56,090] Trial 10 pruned. \n",
      "[I 2025-12-05 17:52:58,935] Trial 11 finished with value: 1.0 and parameters: {'lr_pre': 0.00019316530748794843, 'mask_ratio': 0.443690525951243, 'latent_dim': 128, 'lr_fine': 0.0095138570563802, 'lambda_ent': 0.06861721658130454, 'dropout_rate': 0.2854520975723726}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:53:01,783] Trial 12 finished with value: 1.0 and parameters: {'lr_pre': 0.00030271066897492513, 'mask_ratio': 0.4620349686904057, 'latent_dim': 128, 'lr_fine': 0.0037683752475784157, 'lambda_ent': 0.07538167306444372, 'dropout_rate': 0.2996229756043587}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:53:02,732] Trial 13 pruned. \n",
      "[I 2025-12-05 17:53:03,683] Trial 14 pruned. \n",
      "[I 2025-12-05 17:53:04,630] Trial 15 pruned. \n",
      "[I 2025-12-05 17:53:07,488] Trial 16 finished with value: 0.9919678714859437 and parameters: {'lr_pre': 0.00018008685362547202, 'mask_ratio': 0.6647910877022917, 'latent_dim': 128, 'lr_fine': 0.005992751595887506, 'lambda_ent': 0.08962131416284573, 'dropout_rate': 0.17028764386922235}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:53:10,323] Trial 17 finished with value: 0.9879518072289156 and parameters: {'lr_pre': 0.0014813722261714378, 'mask_ratio': 0.5178544294709914, 'latent_dim': 128, 'lr_fine': 0.002060889904651138, 'lambda_ent': 0.04095669747481726, 'dropout_rate': 0.22428434402227507}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:53:13,122] Trial 18 finished with value: 1.0 and parameters: {'lr_pre': 0.00017036323173820735, 'mask_ratio': 0.3826506740578509, 'latent_dim': 64, 'lr_fine': 0.005955830641447766, 'lambda_ent': 0.13601765288252993, 'dropout_rate': 0.28068703077549195}. Best is trial 1 with value: 1.0.\n",
      "[I 2025-12-05 17:53:14,076] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BEST RESULT: 1.0000\n",
      "BEST PARAMS:\n",
      "  lr_pre: 0.0005317396233045405\n",
      "  mask_ratio: 0.6240695825197669\n",
      "  latent_dim: 128\n",
      "  lr_fine: 0.009936337739842087\n",
      "  lambda_ent: 0.11151781859880357\n",
      "  dropout_rate: 0.1987559624907232\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. RUN OPTIMIZATION\n",
    "# ==========================================\n",
    "print(\"\\n>>> STARTING OPTUNA OPTIMIZATION\")\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"BEST RESULT: {study.best_value:.4f}\")\n",
    "print(\"BEST PARAMS:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
