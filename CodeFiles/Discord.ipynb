{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46bd621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running on: cuda\n",
      ">>> Loading real data from CSV files...\n",
      "    Gene Expression: (265, 44750)\n",
      "    Methylation: (269, 419546)\n",
      "    CNV: (248, 56751)\n",
      "    Labels: (205, 1)\n",
      "    Phenotype: (205, 78)\n",
      "    Labels columns: ['subtype_encoded']\n",
      ">>> Common samples across all datasets: 205\n",
      ">>> NaN check - RNA: 0, Meth: 0, CNV: 0\n",
      ">>> Labels DataFrame info:\n",
      "    Shape: (205, 1)\n",
      "    Columns: ['subtype_encoded']\n",
      "    First few values:\n",
      "                  subtype_encoded\n",
      "TCGA-DX-A48J-01A                2\n",
      "TCGA-QQ-A5VA-01A                0\n",
      "TCGA-DX-AB36-01A                0\n",
      "TCGA-Z4-AAPG-01A                3\n",
      "TCGA-DX-A48N-01A                0\n",
      ">>> Raw labels unique values: [0 1 2 3]\n",
      ">>> Raw labels dtype: int64\n",
      ">>> Label classes (encoded): ['0' '1' '2' '3']\n",
      ">>> Label distribution: {'0': np.int64(53), '1': np.int64(22), '2': np.int64(96), '3': np.int64(34)}\n",
      ">>> Validation:\n",
      "    n_samples: 205\n",
      "    n_classes: 4\n",
      "    Y_labels min/max: 0/3\n",
      "    Y_labels dtype: int64\n",
      ">>> Loaded 205 samples with 4 classes ✓\n",
      ">>> Data loaded successfully! Dimensions: RNA=44750, Meth=419546, CNV=56751\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "# 1. INSTALL OPTUNA IF MISSING\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    print(\"Installing optuna...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION & DEVICE\n",
    "# ==========================================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\">>> Running on: {DEVICE}\")\n",
    "\n",
    "# Enable CUDA debugging for better error messages\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA for debugging\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD REAL DATA FROM CSV FILES\n",
    "# ==========================================\n",
    "print(\">>> Loading real data from CSV files...\")\n",
    "\n",
    "# Load the three omics datasets (transposed so samples are rows)\n",
    "gene_df = pd.read_csv(\"../NewDatasets/processed_expression_4O.csv\", index_col=0).T\n",
    "meth_df = pd.read_csv(\"../NewDatasets/processed_methylation_4O.csv\", index_col=0).T\n",
    "cnv_df  = pd.read_csv(\"../NewDatasets/processed_cnv_4O.csv\", index_col=0).T\n",
    "\n",
    "# Load labels from the labels file\n",
    "labels_df = pd.read_csv(\"../NewDatasets/processed_labels_3Omics_FXS_OG.csv\", index_col=0)\n",
    "\n",
    "# Also load phenotype for reference (can be used if labels file doesn't work)\n",
    "phenotype_df = pd.read_csv(\"../NewDatasets/phenotype_data_clean_FXS_MOFA_3Omics.csv\", index_col=0)\n",
    "\n",
    "print(f\"    Gene Expression: {gene_df.shape}\")\n",
    "print(f\"    Methylation: {meth_df.shape}\")\n",
    "print(f\"    CNV: {cnv_df.shape}\")\n",
    "print(f\"    Labels: {labels_df.shape}\")\n",
    "print(f\"    Phenotype: {phenotype_df.shape}\")\n",
    "print(f\"    Labels columns: {labels_df.columns.tolist()}\")\n",
    "\n",
    "# Find common samples across all datasets\n",
    "common_samples = gene_df.index.intersection(meth_df.index).intersection(cnv_df.index).intersection(labels_df.index)\n",
    "print(f\">>> Common samples across all datasets: {len(common_samples)}\")\n",
    "\n",
    "if len(common_samples) == 0:\n",
    "    print(\">>> ERROR: No common samples found! Checking sample ID formats...\")\n",
    "    print(f\"    Gene samples (first 5): {gene_df.index[:5].tolist()}\")\n",
    "    print(f\"    Meth samples (first 5): {meth_df.index[:5].tolist()}\")\n",
    "    print(f\"    CNV samples (first 5): {cnv_df.index[:5].tolist()}\")\n",
    "    print(f\"    Labels samples (first 5): {labels_df.index[:5].tolist()}\")\n",
    "    raise ValueError(\"No common samples found across datasets!\")\n",
    "\n",
    "# Align all datasets to common samples\n",
    "gene_df = gene_df.loc[common_samples]\n",
    "meth_df = meth_df.loc[common_samples]\n",
    "cnv_df = cnv_df.loc[common_samples]\n",
    "labels_df = labels_df.loc[common_samples]\n",
    "\n",
    "# Convert to numpy arrays and handle NaN values\n",
    "X_rna = gene_df.values.astype(np.float32)\n",
    "X_meth = meth_df.values.astype(np.float32)\n",
    "X_clin = cnv_df.values.astype(np.float32)  # Using CNV as the third modality\n",
    "\n",
    "# Check for NaN values in features\n",
    "print(f\">>> NaN check - RNA: {np.isnan(X_rna).sum()}, Meth: {np.isnan(X_meth).sum()}, CNV: {np.isnan(X_clin).sum()}\")\n",
    "\n",
    "# Replace NaN with 0 (or mean imputation could be done)\n",
    "X_rna = np.nan_to_num(X_rna, nan=0.0)\n",
    "X_meth = np.nan_to_num(X_meth, nan=0.0)\n",
    "X_clin = np.nan_to_num(X_clin, nan=0.0)\n",
    "\n",
    "# Extract labels - handle different possible column structures\n",
    "print(f\">>> Labels DataFrame info:\")\n",
    "print(f\"    Shape: {labels_df.shape}\")\n",
    "print(f\"    Columns: {labels_df.columns.tolist()}\")\n",
    "print(f\"    First few values:\\n{labels_df.head()}\")\n",
    "\n",
    "if labels_df.shape[1] == 1:\n",
    "    # Single column of labels\n",
    "    raw_labels = labels_df.iloc[:, 0].values\n",
    "else:\n",
    "    # Multiple columns - use the first one or look for common label column names\n",
    "    label_cols = [col for col in labels_df.columns if col.lower() in ['label', 'class', 'target', 'y', 'subtype', 'group', 'category']]\n",
    "    if label_cols:\n",
    "        raw_labels = labels_df[label_cols[0]].values\n",
    "        print(f\">>> Using label column: {label_cols[0]}\")\n",
    "    else:\n",
    "        raw_labels = labels_df.iloc[:, 0].values\n",
    "        print(f\">>> Using first column as labels: {labels_df.columns[0]}\")\n",
    "\n",
    "print(f\">>> Raw labels unique values: {np.unique(raw_labels)}\")\n",
    "print(f\">>> Raw labels dtype: {raw_labels.dtype}\")\n",
    "\n",
    "# Always use LabelEncoder to ensure labels are 0-indexed consecutive integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Convert to string first to handle any type, and filter out NaN if present\n",
    "raw_labels_str = pd.Series(raw_labels).fillna('UNKNOWN').astype(str).values\n",
    "Y_labels = le.fit_transform(raw_labels_str)\n",
    "\n",
    "print(f\">>> Label classes (encoded): {le.classes_}\")\n",
    "print(f\">>> Label distribution: {dict(zip(le.classes_, np.bincount(Y_labels)))}\")\n",
    "\n",
    "# Validate labels\n",
    "n_samples = len(Y_labels)\n",
    "n_classes = len(le.classes_)\n",
    "print(f\">>> Validation:\")\n",
    "print(f\"    n_samples: {n_samples}\")\n",
    "print(f\"    n_classes: {n_classes}\")\n",
    "print(f\"    Y_labels min/max: {Y_labels.min()}/{Y_labels.max()}\")\n",
    "print(f\"    Y_labels dtype: {Y_labels.dtype}\")\n",
    "\n",
    "assert Y_labels.min() >= 0, f\"Labels must be >= 0, got min: {Y_labels.min()}\"\n",
    "assert Y_labels.max() < n_classes, f\"Labels must be < n_classes ({n_classes}), got max: {Y_labels.max()}\"\n",
    "print(f\">>> Loaded {n_samples} samples with {n_classes} classes ✓\")\n",
    "\n",
    "# Scale the data\n",
    "scaler_rna = StandardScaler()\n",
    "scaler_meth = StandardScaler()\n",
    "scaler_clin = StandardScaler()\n",
    "\n",
    "X_rna = scaler_rna.fit_transform(X_rna)\n",
    "X_meth = scaler_meth.fit_transform(X_meth)\n",
    "X_clin = scaler_clin.fit_transform(X_clin)\n",
    "\n",
    "# Final NaN check after scaling\n",
    "assert not np.isnan(X_rna).any(), \"NaN found in X_rna after scaling!\"\n",
    "assert not np.isnan(X_meth).any(), \"NaN found in X_meth after scaling!\"\n",
    "assert not np.isnan(X_clin).any(), \"NaN found in X_clin after scaling!\"\n",
    "\n",
    "# Get dimensions for model initialization\n",
    "DIMS = (X_rna.shape[1], X_meth.shape[1], X_clin.shape[1])\n",
    "print(f\">>> Data loaded successfully! Dimensions: RNA={DIMS[0]}, Meth={DIMS[1]}, CNV={DIMS[2]}\")\n",
    "\n",
    "# ==========================================\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL CLASSES\n",
    "# ==========================================\n",
    "# ==========================================\n",
    "# 2. MODEL CLASSES (FIXED)\n",
    "# ==========================================\n",
    "class PerOmicCMAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LayerNorm(512), nn.GELU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512), nn.GELU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim), nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask_ratio=0.0):\n",
    "        if mask_ratio > 0 and self.training:\n",
    "            mask = (torch.rand_like(x) > mask_ratio).float()\n",
    "            x_masked = x * mask\n",
    "        else:\n",
    "            mask = torch.ones_like(x)\n",
    "            x_masked = x\n",
    "        z = self.encoder(x_masked)\n",
    "\n",
    "        # --- FIX: RETURN 4 VALUES (Added 'mask') ---\n",
    "        return self.decoder(z), self.projector(z), z, mask\n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(self, latent_dim,  n_classes=3, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.gate_rna = nn.Linear(latent_dim, 1)\n",
    "        self.gate_meth = nn.Linear(latent_dim, 1)\n",
    "        self.gate_clin = nn.Linear(latent_dim, 1)\n",
    "        self.classifier = nn.Linear(latent_dim, n_classes)\n",
    "        self.drop_rate = dropout_rate\n",
    "\n",
    "    def forward(self, z_rna, z_meth, z_clin, apply_dropout=False):\n",
    "        if apply_dropout and self.training:\n",
    "            if torch.rand(1).item() < self.drop_rate: z_rna = torch.zeros_like(z_rna)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_meth = torch.zeros_like(z_meth)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_clin = torch.zeros_like(z_clin)\n",
    "\n",
    "        w_rna = torch.sigmoid(self.gate_rna(z_rna))\n",
    "        w_meth = torch.sigmoid(self.gate_meth(z_meth))\n",
    "        w_clin = torch.sigmoid(self.gate_clin(z_clin))\n",
    "\n",
    "        z_fused = (w_rna * z_rna + w_meth * z_meth + w_clin * z_clin) / (w_rna + w_meth + w_clin + 1e-8)\n",
    "        return self.classifier(z_fused), torch.cat([w_rna, w_meth, w_clin], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de12e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StabilizedUncertaintyLoss(nn.Module):\n",
    "    def __init__(self, num_losses):\n",
    "        super().__init__()\n",
    "        self.log_vars = nn.Parameter(torch.zeros(num_losses))\n",
    "    def forward(self, losses):\n",
    "        total = 0\n",
    "        for i, loss in enumerate(losses):\n",
    "            prec = torch.clamp(0.5 * torch.exp(-self.log_vars[i]), 0.2, 3.0)\n",
    "            total += prec * loss + 0.5 * self.log_vars[i]\n",
    "        return total\n",
    "\n",
    "def contrastive_loss(q, k, queue, temp=0.1):\n",
    "    q = F.normalize(q, dim=1); k = F.normalize(k, dim=1); queue = queue.detach()\n",
    "    l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "    l_neg = torch.einsum('nc,ck->nk', [q, queue])\n",
    "    logits = torch.cat([l_pos, l_neg], dim=1) / temp\n",
    "    return F.cross_entropy(logits, torch.zeros(logits.shape[0], dtype=torch.long).to(q.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a6005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. OPTUNA OBJECTIVE FUNCTION (FIXED)\n",
    "# ==========================================\n",
    "def objective(trial):\n",
    "    # --- A. Suggest Hyperparameters ---\n",
    "    lr_pre = trial.suggest_float(\"lr_pre\", 1e-4, 5e-3, log=True)\n",
    "    mask_ratio = trial.suggest_float(\"mask_ratio\", 0.25, 0.75)\n",
    "    latent_dim = trial.suggest_categorical(\"latent_dim\", [64, 128])\n",
    "\n",
    "    lr_fine = trial.suggest_float(\"lr_fine\", 1e-4, 1e-2, log=True)\n",
    "    lambda_ent = trial.suggest_float(\"lambda_ent\", 0.0, 0.2)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4)\n",
    "\n",
    "    # --- B. Cross-Validation Loop ---\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_rna, Y_labels)):\n",
    "\n",
    "        # 1. Prepare Data\n",
    "        tr_ds = TensorDataset(\n",
    "            torch.FloatTensor(X_rna[train_idx]), torch.FloatTensor(X_meth[train_idx]),\n",
    "            torch.FloatTensor(X_clin[train_idx]), torch.LongTensor(Y_labels[train_idx])\n",
    "        )\n",
    "        val_ds = TensorDataset(\n",
    "            torch.FloatTensor(X_rna[val_idx]), torch.FloatTensor(X_meth[val_idx]),\n",
    "            torch.FloatTensor(X_clin[val_idx]), torch.LongTensor(Y_labels[val_idx])\n",
    "        )\n",
    "\n",
    "        tr_loader = DataLoader(tr_ds, batch_size=32, shuffle=True, drop_last=True)\n",
    "        val_rna, val_meth, val_clin, val_y = val_ds[:]\n",
    "        val_rna, val_meth, val_clin, val_y = val_rna.to(DEVICE), val_meth.to(DEVICE), val_clin.to(DEVICE), val_y.to(DEVICE)\n",
    "\n",
    "        # 2. Init Models\n",
    "        cmae_r = PerOmicCMAE(DIMS[0], latent_dim).to(DEVICE)\n",
    "        cmae_m = PerOmicCMAE(DIMS[1], latent_dim).to(DEVICE)\n",
    "        cmae_c = PerOmicCMAE(DIMS[2], latent_dim).to(DEVICE)\n",
    "        mem_bank = nn.Parameter(F.normalize(torch.randn(latent_dim, 128), dim=0), requires_grad=False).to(DEVICE)\n",
    "        loss_fn = StabilizedUncertaintyLoss(4).to(DEVICE)\n",
    "\n",
    "        opt_pre = optim.AdamW(list(cmae_r.parameters())+list(cmae_m.parameters())+list(cmae_c.parameters())+list(loss_fn.parameters()), lr=lr_pre)\n",
    "\n",
    "        # 3. Phase 1: Pre-training\n",
    "        cmae_r.train(); cmae_m.train(); cmae_c.train()\n",
    "        for epoch in range(15): # Reduced epochs for speed\n",
    "            for r, m, c, _ in tr_loader:\n",
    "                r, m, c = r.to(DEVICE), m.to(DEVICE), c.to(DEVICE)\n",
    "\n",
    "                # Forward View 1 (Now unpacking 4 values works)\n",
    "                rec_r1, proj_r1, _, _ = cmae_r(r, mask_ratio)\n",
    "                rec_m1, proj_m1, _, _ = cmae_m(m, mask_ratio)\n",
    "                rec_c1, proj_c1, _, _ = cmae_c(c, mask_ratio)\n",
    "\n",
    "                # Forward View 2\n",
    "                with torch.no_grad():\n",
    "                    _, proj_r2, _, _ = cmae_r(r, mask_ratio)\n",
    "                    _, proj_m2, _, _ = cmae_m(m, mask_ratio)\n",
    "                    _, proj_c2, _, _ = cmae_c(c, mask_ratio)\n",
    "\n",
    "                loss = loss_fn([\n",
    "                    F.mse_loss(rec_r1, r), F.mse_loss(rec_m1, m), F.mse_loss(rec_c1, c),\n",
    "                    (contrastive_loss(proj_r1, proj_r2, mem_bank) +\n",
    "                     contrastive_loss(proj_m1, proj_m2, mem_bank) +\n",
    "                     contrastive_loss(proj_c1, proj_c2, mem_bank))/3\n",
    "                ])\n",
    "\n",
    "                opt_pre.zero_grad(); loss.backward(); opt_pre.step()\n",
    "                with torch.no_grad():\n",
    "                    avg_proj = (proj_r1 + proj_m1 + proj_c1) / 3\n",
    "                    mem_bank.data = torch.cat([mem_bank[:, avg_proj.shape[0]:], avg_proj.T], dim=1)\n",
    "\n",
    "        # 4. Phase 2: Fine-tuning\n",
    "        cmae_r.eval(); cmae_m.eval(); cmae_c.eval()\n",
    "        fusion = GatedAttentionFusion(latent_dim, n_classes=n_classes, dropout_rate=dropout_rate).to(DEVICE)\n",
    "        opt_fine = optim.AdamW(fusion.parameters(), lr=lr_fine)\n",
    "\n",
    "        best_fold_acc = 0\n",
    "\n",
    "        for epoch in range(20):\n",
    "            fusion.train()\n",
    "            for r, m, c, y in tr_loader:\n",
    "                r, m, c, y = r.to(DEVICE), m.to(DEVICE), c.to(DEVICE), y.to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    # --- FIX: UNPACK 4 VALUES HERE TOO (using _) ---\n",
    "                    _, _, zr, _ = cmae_r(r)\n",
    "                    _, _, zm, _ = cmae_m(m)\n",
    "                    _, _, zc, _ = cmae_c(c)\n",
    "\n",
    "                logits, weights = fusion(zr, zm, zc, apply_dropout=True)\n",
    "                cls_loss = F.cross_entropy(logits, y)\n",
    "                entropy = -torch.sum(weights * torch.log(weights + 1e-8), dim=1).mean()\n",
    "                loss = cls_loss + lambda_ent * entropy\n",
    "\n",
    "                opt_fine.zero_grad(); loss.backward(); opt_fine.step()\n",
    "\n",
    "            fusion.eval()\n",
    "            with torch.no_grad():\n",
    "                # --- FIX: UNPACK 4 VALUES HERE TOO ---\n",
    "                _, _, zr, _ = cmae_r(val_rna)\n",
    "                _, _, zm, _ = cmae_m(val_meth)\n",
    "                _, _, zc, _ = cmae_c(val_clin)\n",
    "\n",
    "                logits, _ = fusion(zr, zm, zc, apply_dropout=False)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                acc = accuracy_score(val_y.cpu(), preds.cpu())\n",
    "                if acc > best_fold_acc: best_fold_acc = acc\n",
    "\n",
    "        fold_accuracies.append(best_fold_acc)\n",
    "\n",
    "        trial.report(np.mean(fold_accuracies), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return np.mean(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0b2e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-13 21:08:56,723] A new study created in memory with name: no-name-2afe6a8c-fca1-4a14-85ff-377b2f97534b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> STARTING OPTUNA OPTIMIZATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-13 21:10:02,339] Trial 0 finished with value: 0.4690252912759307 and parameters: {'lr_pre': 0.00029834963985974554, 'mask_ratio': 0.515033869324458, 'latent_dim': 128, 'lr_fine': 0.00010871800782359245, 'lambda_ent': 0.11824115023910386, 'dropout_rate': 0.25068008872908576}. Best is trial 0 with value: 0.4690252912759307.\n",
      "[I 2025-12-13 21:10:54,986] Trial 1 finished with value: 0.7659846547314578 and parameters: {'lr_pre': 0.00013046904956839784, 'mask_ratio': 0.6240765739682311, 'latent_dim': 128, 'lr_fine': 0.009065487705290745, 'lambda_ent': 0.04233533196028845, 'dropout_rate': 0.17980413408933543}. Best is trial 1 with value: 0.7659846547314578.\n",
      "[I 2025-12-13 21:11:47,440] Trial 2 finished with value: 0.6243961352657005 and parameters: {'lr_pre': 0.0007499530738967994, 'mask_ratio': 0.3885556942987851, 'latent_dim': 128, 'lr_fine': 0.00036141843950902554, 'lambda_ent': 0.1824531363011566, 'dropout_rate': 0.19893203331056675}. Best is trial 1 with value: 0.7659846547314578.\n",
      "[I 2025-12-13 21:12:39,659] Trial 3 finished with value: 0.7220090934924693 and parameters: {'lr_pre': 0.0016003317080001125, 'mask_ratio': 0.28962174355892706, 'latent_dim': 64, 'lr_fine': 0.007279362779774347, 'lambda_ent': 0.17634454095673788, 'dropout_rate': 0.38862289679134876}. Best is trial 1 with value: 0.7659846547314578.\n",
      "[I 2025-12-13 21:13:31,915] Trial 4 finished with value: 0.7756464904802499 and parameters: {'lr_pre': 0.0003903243314870362, 'mask_ratio': 0.5558342849888231, 'latent_dim': 64, 'lr_fine': 0.006238387697180946, 'lambda_ent': 0.0661729444257331, 'dropout_rate': 0.31028833058920263}. Best is trial 4 with value: 0.7756464904802499.\n",
      "[I 2025-12-13 21:13:49,418] Trial 5 pruned. \n",
      "[I 2025-12-13 21:14:07,034] Trial 6 pruned. \n",
      "[I 2025-12-13 21:17:36,055] Trial 7 pruned. \n",
      "[W 2025-12-13 21:17:42,660] Trial 8 failed with parameters: {'lr_pre': 0.00022510768093327618, 'mask_ratio': 0.6273503318110507, 'latent_dim': 64, 'lr_fine': 0.00016068558154182108, 'lambda_ent': 0.06503253780204923, 'dropout_rate': 0.3860304778460538} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 18.37 GiB is allocated by PyTorch, and 233.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\shini\\AppData\\Local\\Temp\\ipykernel_14728\\777729344.py\", line 67, in objective\n",
      "    opt_pre.zero_grad(); loss.backward(); opt_pre.step()\n",
      "                                          ~~~~~~~~~~~~^^\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 516, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 81, in _use_grad\n",
      "    ret = func(*args, **kwargs)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\adam.py\", line 247, in step\n",
      "    adam(\n",
      "    ~~~~^\n",
      "        params_with_grad,\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "    ...<19 lines>...\n",
      "        decoupled_weight_decay=group[\"decoupled_weight_decay\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 149, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\adam.py\", line 949, in adam\n",
      "    func(\n",
      "    ~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<17 lines>...\n",
      "        decoupled_weight_decay=decoupled_weight_decay,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\adam.py\", line 773, in _multi_tensor_adam\n",
      "    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 18.37 GiB is allocated by PyTorch, and 233.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[W 2025-12-13 21:17:42,749] Trial 8 failed with value None.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 18.37 GiB is allocated by PyTorch, and 233.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> STARTING OPTUNA OPTIMIZATION\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m, pruner=optuna.pruners.MedianPruner())\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBEST RESULT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     58\u001b[39m     _, proj_c2, _, _ = cmae_c(c, mask_ratio)\n\u001b[32m     60\u001b[39m loss = loss_fn([\n\u001b[32m     61\u001b[39m     F.mse_loss(rec_r1, r), F.mse_loss(rec_m1, m), F.mse_loss(rec_c1, c),\n\u001b[32m     62\u001b[39m     (contrastive_loss(proj_r1, proj_r2, mem_bank) +\n\u001b[32m     63\u001b[39m      contrastive_loss(proj_m1, proj_m2, mem_bank) +\n\u001b[32m     64\u001b[39m      contrastive_loss(proj_c1, proj_c2, mem_bank))/\u001b[32m3\u001b[39m\n\u001b[32m     65\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m opt_pre.zero_grad(); loss.backward(); \u001b[43mopt_pre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     69\u001b[39m     avg_proj = (proj_r1 + proj_m1 + proj_c1) / \u001b[32m3\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\torch\\optim\\adam.py:773\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    771\u001b[39m     exp_avg_sq_sqrt = torch._foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     exp_avg_sq_sqrt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[32m    776\u001b[39m torch._foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 18.37 GiB is allocated by PyTorch, and 233.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. RUN OPTIMIZATION\n",
    "# ==========================================\n",
    "print(\"\\n>>> STARTING OPTUNA OPTIMIZATION\")\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"BEST RESULT: {study.best_value:.4f}\")\n",
    "print(\"BEST PARAMS:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
