import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.metrics import f1_score, classification_report, silhouette_score
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt

# ==================================================================================
# 1. CONFIGURATION
# ==================================================================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

CONFIG = {
    'latent_dim': 64,          # Smaller latent space for small N=205
    'dropout': 0.5,            # High dropout to prevent memorization
    'lr': 0.0005,              # Lower learning rate for stability
    'weight_decay': 0.01,      # Strong regularization
    'epochs': 1000,
    'warmup_epochs': 150,      # Phase 1 duration
    'n_features': 5000,        # Top K features to keep per modality (Crucial!)
    'n_neighbors_impute': 5    # For KNN Imputation
}

# ==================================================================================
# 2. DATA LOADING & PREPROCESSING (The "Safety" Block)
# ==================================================================================
def preprocess_omics(gene_path, meth_path, cnv_path, label_path):
    print(">>> Loading Data...")
    # Load your data (Transposing to Samples x Features)
    gene = pd.read_csv(gene_path, index_col=0).T
    meth = pd.read_csv(meth_path, index_col=0).T
    cnv  = pd.read_csv(cnv_path, index_col=0).T
    labels = pd.read_csv(label_path, index_col=0) # Assuming 'subtype' column exists
    
    # 1. Align Samples (Intersection)
    common_samples = gene.index.intersection(meth.index).intersection(cnv.index).intersection(labels.index)
    print(f"Aligned {len(common_samples)} samples across all modalities.")
    
    gene = gene.loc[common_samples]
    meth = meth.loc[common_samples]
    cnv  = cnv.loc[common_samples]
    y = labels.loc[common_samples] # Adjust column name if needed e.g. y['subtype']
    
    # 2. Feature Selection (Variance Threshold)
    # We MUST reduce 500k features to avoid overfitting/crashing
    print(f">>> Filtering Top {CONFIG['n_features']} Variable Features...")
    
    def select_features(df, n):
        variances = df.var()
        top_feats = variances.sort_values(ascending=False).head(n).index
        return df[top_feats]

    gene = select_features(gene, CONFIG['n_features'])
    meth = select_features(meth, CONFIG['n_features'])
    cnv  = select_features(cnv, CONFIG['n_features'])

    # 3. KNN Imputation (Handling NaNs natively via preprocessing)
    print(">>> Imputing Missing Values (KNN)...")
    imputer = KNNImputer(n_neighbors=CONFIG['n_neighbors_impute'])
    
    # Impute and turn back into DF to keep columns
    gene = pd.DataFrame(imputer.fit_transform(gene), index=gene.index, columns=gene.columns)
    meth = pd.DataFrame(imputer.fit_transform(meth), index=meth.index, columns=meth.columns)
    cnv  = pd.DataFrame(imputer.fit_transform(cnv), index=cnv.index, columns=cnv.columns)

    # 4. Standard Scaling
    print(">>> Scaling Data...")
    scaler = StandardScaler()
    gene = scaler.fit_transform(gene)
    meth = scaler.fit_transform(meth)
    cnv  = scaler.fit_transform(cnv)
    
    # Convert Labels to Integers
    # Assuming labels are strings, we map them
    unique_classes = sorted(y.iloc[:,0].unique())
    class_map = {label: i for i, label in enumerate(unique_classes)}
    y_encoded = y.iloc[:,0].map(class_map).values
    
    print("Data Ready.")
    return (
        torch.FloatTensor(gene), 
        torch.FloatTensor(meth), 
        torch.FloatTensor(cnv), 
        torch.LongTensor(y_encoded),
        len(unique_classes),
        unique_classes
    )

# ==================================================================================
# 3. ARCHITECTURE: Joint Prototypical Autoencoder
# ==================================================================================
class PrototypicalHead(nn.Module):
    def __init__(self, in_features, num_classes):
        super().__init__()
        # Learnable "Centers" for each class
        self.prototypes = nn.Parameter(torch.randn(num_classes, in_features))

    def forward(self, z):
        # Calculate Squared Euclidean Distance
        # |x-y|^2 = x^2 + y^2 - 2xy
        z_sq = torch.sum(z**2, dim=1, keepdim=True)
        p_sq = torch.sum(self.prototypes**2, dim=1)
        zp   = torch.mm(z, self.prototypes.t())
        
        distances = z_sq + p_sq - 2 * zp
        
        # Return negative distance (closer = higher logit)
        return -distances

class JointOmicsProtoAE(nn.Module):
    def __init__(self, input_dims, latent_dim=128, num_classes=3, dropout=0.5):
        super().__init__()
        
        # --- Modality Encoders ---
        self.enc_gene = nn.Sequential(
            nn.Linear(input_dims['gene'], 1024),
            nn.BatchNorm1d(1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(dropout)
        )
        self.enc_meth = nn.Sequential(
            nn.Linear(input_dims['meth'], 1024),
            nn.BatchNorm1d(1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(dropout)
        )
        self.enc_cnv = nn.Sequential(
            nn.Linear(input_dims['cnv'], 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Dropout(dropout)
        )
        
        # --- Shared Trunk (Intermediate Fusion) ---
        self.shared_encoder = nn.Sequential(
            nn.Linear(2560, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Dropout(dropout),
            nn.Linear(512, latent_dim) 
            # Output is NOT normalized yet; we do it in forward()
        )
        
        # --- Heads ---
        self.proto_head = PrototypicalHead(latent_dim, num_classes)
        
        # --- Decoders ---
        self.dec_gene = nn.Sequential(nn.Linear(latent_dim, input_dims['gene']))
        self.dec_meth = nn.Sequential(nn.Linear(latent_dim, input_dims['meth']))
        self.dec_cnv  = nn.Sequential(nn.Linear(latent_dim, input_dims['cnv']))

    def forward(self, x_gene, x_meth, x_cnv):
        # 1. Individual Encoding
        h_g = self.enc_gene(x_gene)
        h_m = self.enc_meth(x_meth)
        h_c = self.enc_cnv(x_cnv)
        
        # 2. Intermediate Fusion
        fused = torch.cat([h_g, h_m, h_c], dim=1)
        
        # 3. Latent Space Projection
        z_raw = self.shared_encoder(fused)
        # L2 Normalize z (Critical for Prototypical Networks)
        z = F.normalize(z_raw, p=2, dim=1)
        
        # 4. Classification
        logits = self.proto_head(z)
        
        # 5. Reconstruction
        r_g = self.dec_gene(z)
        r_m = self.dec_meth(z)
        r_c = self.dec_cnv(z)
        
        return r_g, r_m, r_c, logits, z

# ==================================================================================
# 4. UTILITIES & METRICS
# ==================================================================================
def evaluate_metrics(model, g, m, c, y, class_names):
    model.eval()
    with torch.no_grad():
        _, _, _, logits, z = model(g, m, c)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        labels = y.cpu().numpy()
        latent = z.cpu().numpy()
        
    macro_f1 = f1_score(labels, preds, average='macro')
    sil_score = silhouette_score(latent, labels)
    
    print("\n" + "="*40)
    print(" >>> EVALUATION REPORT")
    print("="*40)
    print(f"Macro F1-Score:     {macro_f1:.4f}")
    print(f"Silhouette Score:   {sil_score:.4f}")
    print("-" * 40)
    print(classification_report(labels, preds, target_names=class_names, zero_division=0))
    print("="*40 + "\n")
    return macro_f1

# ==================================================================================
# 5. TRAINING STRATEGY (Two-Phase)
# ==================================================================================
def train_pipeline(model, g, m, c, y, class_names):
    # Calculate Class Weights for Imbalance
    class_weights = compute_class_weight('balanced', classes=np.unique(y.cpu().numpy()), y=y.cpu().numpy())
    weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)
    
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)
    
    print(f"Starting Training for {CONFIG['epochs']} epochs...")
    print(f"Phase 1: Warm-Up (0-{CONFIG['warmup_epochs']} epochs) - Reconstruction Only")
    print(f"Phase 2: Joint (>{CONFIG['warmup_epochs']} epochs) - Prototypical Classification Active")
    
    history = []
    
    for epoch in range(CONFIG['epochs']):
        model.train()
        optimizer.zero_grad()
        
        # Forward
        rg, rm, rc, logits, z = model(g, m, c)
        
        # Losses
        loss_recon = F.mse_loss(rg, g) + F.mse_loss(rm, m) + F.mse_loss(rc, c)
        loss_class = F.cross_entropy(logits, y, weight=weights_tensor)
        
        # STRATEGY: Two-Phase Switching
        if epoch < CONFIG['warmup_epochs']:
            total_loss = loss_recon
            phase = "WARMUP"
        else:
            # Heavily weight classification to force separation
            total_loss = loss_recon + (5.0 * loss_class)
            phase = "JOINT "
            
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        # Monitoring
        if epoch % 50 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch:4d} [{phase}] | Loss: {total_loss.item():.4f} | Recon: {loss_recon.item():.4f} | Class: {loss_class.item():.4f} | LR: {current_lr:.1e}")
            scheduler.step(total_loss)
            
    return model

# ==================================================================================
# 6. MAIN EXECUTION GUARD
# ==================================================================================
if __name__ == "__main__":
    # --- STEP 1: DEFINE FILE PATHS (UPDATE THESE!) ---
    # Assuming standard structure based on your notebook
    G_PATH = "../NewDatasets/processed_expression_4O.csv"
    M_PATH = "../NewDatasets/processed_methylation_4O.csv"
    C_PATH = "../NewDatasets/processed_cnv_4O.csv"
    L_PATH = "../NewDatasets/processed_labels_3Omics_FXS_OG.csv" 
    
    try:
        # --- STEP 2: LOAD & PREPROCESS ---
        g_tensor, m_tensor, c_tensor, y_tensor, n_classes, class_names = preprocess_omics(G_PATH, M_PATH, C_PATH, L_PATH)
        
        # Move to GPU
        g_tensor = g_tensor.to(DEVICE)
        m_tensor = m_tensor.to(DEVICE)
        c_tensor = c_tensor.to(DEVICE)
        y_tensor = y_tensor.to(DEVICE)
        
        # --- STEP 3: INITIALIZE MODEL ---
        input_dims = {
            'gene': g_tensor.shape[1],
            'meth': m_tensor.shape[1],
            'cnv':  c_tensor.shape[1]
        }
        
        model = JointOmicsProtoAE(
            input_dims=input_dims, 
            latent_dim=CONFIG['latent_dim'], 
            num_classes=n_classes,
            dropout=CONFIG['dropout']
        ).to(DEVICE)
        
        # --- STEP 4: TRAIN ---
        model = train_pipeline(model, g_tensor, m_tensor, c_tensor, y_tensor, class_names)
        
        # --- STEP 5: FINAL EVALUATION ---
        evaluate_metrics(model, g_tensor, m_tensor, c_tensor, y_tensor, class_names)
        
        # Optional: Save Model
        torch.save(model.state_dict(), "JointProtoAE_Best.pth")
        print("Model saved to JointProtoAE_Best.pth")
        
    except FileNotFoundError as e:
        print(f"\n[ERROR] Files not found. Please update the G_PATH, M_PATH, etc. in the 'if __name__' block.\nDetails: {e}")
    except Exception as e:
        print(f"\n[ERROR] An error occurred:\n{e}")