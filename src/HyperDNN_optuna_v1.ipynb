{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5c17cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import optuna\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\">>> Running on: {DEVICE}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "FEATURE_COUNTS = [1500, 2000, 2500, 3000, 4000, 5000]\n",
    "SUBTYPES_OF_INTEREST = [\n",
    "    'Leiomyosarcoma, NOS',\n",
    "    'Dedifferentiated liposarcoma',\n",
    "    'Undifferentiated sarcoma',\n",
    "    'Fibromyxosarcoma'\n",
    "]\n",
    "N_TRIALS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec88bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_aligned_data():\n",
    "    print(f\"\\n>>> LOADING RAW ALIGNED DATA\")\n",
    "    \n",
    "    # 1. Load Phenotype/Labels\n",
    "    pheno_path = \"../Data/phenotype_clean.csv\"\n",
    "    if not os.path.exists(pheno_path):\n",
    "        raise FileNotFoundError(f\"{pheno_path} not found.\")\n",
    "    \n",
    "    pheno = pd.read_csv(pheno_path, index_col=0)\n",
    "    \n",
    "    col_name = 'primary_diagnosis.diagnoses'\n",
    "    if col_name not in pheno.columns:\n",
    "        print(f\"Warning: '{col_name}' not found.\")\n",
    "        return None\n",
    "        \n",
    "    mask = pheno[col_name].isin(SUBTYPES_OF_INTEREST)\n",
    "    pheno = pheno[mask]\n",
    "    print(f\"  Phenotype Samples (filtered): {pheno.shape[0]}\")\n",
    "\n",
    "    # 2. Load Omics\n",
    "    def load_omic(path, name):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: {path} not found.\")\n",
    "            return None\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df = df.T # samples x features\n",
    "        return df\n",
    "\n",
    "    rna = load_omic(\"../Data/expression_log.csv\", \"RNA (Expression)\")\n",
    "    meth = load_omic(\"../Data/methylation_mvalues.csv\", \"Methylation\")\n",
    "    cnv = load_omic(\"../Data/cnv_log.csv\", \"CNV\")\n",
    "    \n",
    "    if rna is None or meth is None or cnv is None:\n",
    "        raise ValueError(\"One or more omics files missing.\")\n",
    "\n",
    "    # 3. Intersection\n",
    "    common_samples = pheno.index.intersection(rna.index).intersection(meth.index).intersection(cnv.index)\n",
    "    print(f\"  Common Samples: {len(common_samples)}\")\n",
    "    \n",
    "    if len(common_samples) == 0:\n",
    "        raise ValueError(\"No common samples found!\")\n",
    "\n",
    "    pheno = pheno.loc[common_samples]\n",
    "    rna = rna.loc[common_samples]\n",
    "    meth = meth.loc[common_samples]\n",
    "    cnv = cnv.loc[common_samples]\n",
    "    \n",
    "    # 4. Prepare Labels\n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(pheno[col_name])\n",
    "    print(f\"  Classes: {le.classes_}\")\n",
    "\n",
    "    return rna, meth, cnv, Y, le.classes_\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c734e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankHyperNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, in_features, out_features, rank=16):\n",
    "        super(LowRankHyperNetwork, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        \n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.U_generator = nn.Linear(hidden_dim, in_features * rank)\n",
    "        self.V_generator = nn.Linear(hidden_dim, rank * out_features)\n",
    "        self.bias_generator = nn.Linear(hidden_dim, out_features)\n",
    "        self.scale = nn.Parameter(torch.ones(1) * 0.1)\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "        features = self.backbone(embedding)\n",
    "        U = self.U_generator(features).view(self.in_features, self.rank)\n",
    "        V = self.V_generator(features).view(self.rank, self.out_features)\n",
    "        weight = (U @ V).T * self.scale\n",
    "        bias = self.bias_generator(features)\n",
    "        return weight, bias\n",
    "\n",
    "class HyperEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim, embedding_dim, \n",
    "                 hyper_hidden_dim=64, rank=16, dropout=0.3):\n",
    "        super(HyperEncoder, self).__init__()\n",
    "        self.hyper_layer = LowRankHyperNetwork(\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dim=hyper_hidden_dim,\n",
    "            in_features=input_dim,\n",
    "            out_features=hidden_dims[0],\n",
    "            rank=rank\n",
    "        )\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        prev_dim = hidden_dims[0]\n",
    "        for hidden_dim in hidden_dims[1:]:\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "        self.final_layer = nn.Linear(prev_dim, latent_dim)\n",
    "        self.first_bn = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, embedding):\n",
    "        weight, bias = self.hyper_layer(embedding)\n",
    "        x = F.linear(x, weight, bias)\n",
    "        x = self.first_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer, bn in zip(self.layers, self.batch_norms):\n",
    "            x = layer(x) # bn(layer(x)) usually better, adjusted order here\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        z = self.final_layer(x)\n",
    "        return z\n",
    "\n",
    "class HyperDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dims, output_dim, dropout=0.3):\n",
    "        super(HyperDecoder, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class HyperDNN(nn.Module):\n",
    "    def __init__(self, input_dims, latent_dim=32, hyper_hidden_dim=64, \n",
    "                 embedding_dim=16, encoder_hidden_dims=[128, 64], \n",
    "                 rank=16, dropout=0.4):\n",
    "        super(HyperDNN, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.omics_names = list(input_dims.keys())\n",
    "        self.num_omics = len(self.omics_names)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.omics_embeddings = nn.Embedding(self.num_omics, embedding_dim)\n",
    "        self.encoders = nn.ModuleDict()\n",
    "        self.decoders = nn.ModuleDict()\n",
    "        for name, input_dim in input_dims.items():\n",
    "            self.encoders[name] = HyperEncoder(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dims=encoder_hidden_dims,\n",
    "                latent_dim=latent_dim,\n",
    "                embedding_dim=embedding_dim,\n",
    "                hyper_hidden_dim=hyper_hidden_dim,\n",
    "                rank=rank,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            self.decoders[name] = HyperDecoder(\n",
    "                latent_dim=latent_dim,\n",
    "                hidden_dims=list(reversed(encoder_hidden_dims)),\n",
    "                output_dim=input_dim,\n",
    "                dropout=dropout\n",
    "            )\n",
    "        for i, name in enumerate(self.omics_names):\n",
    "            self.register_buffer(f'omics_idx_{name}', torch.tensor(i))\n",
    "    \n",
    "    def get_omics_embedding(self, omics_name, device):\n",
    "        idx = getattr(self, f'omics_idx_{omics_name}')\n",
    "        return self.omics_embeddings(idx)\n",
    "    \n",
    "    def encode(self, x, omics_name):\n",
    "        embedding = self.get_omics_embedding(omics_name, x.device)\n",
    "        z = self.encoders[omics_name](x, embedding)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, omics_name):\n",
    "        return self.decoders[omics_name](z)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        latents = {}\n",
    "        reconstructions = {}\n",
    "        for name, x in inputs.items():\n",
    "            z = self.encode(x, name)\n",
    "            recon = self.decode(z, name)\n",
    "            latents[name] = z\n",
    "            reconstructions[name] = recon\n",
    "        return latents, reconstructions\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3622d062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_evaluation(params, n_features, rna_df, meth_df, cnv_df, Y, class_names, is_optuna=True):\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_metrics = {'accuracy': [], 'f1_macro': [], 'f1_micro': [], 'precision': [], 'recall': []}\n",
    "    \n",
    "    # Hyperparams extraction\n",
    "    latent_dim = params['latent_dim']\n",
    "    hyper_hidden = params['hyper_hidden_dim']\n",
    "    embed_dim = params['embedding_dim']\n",
    "    rank = params['rank']\n",
    "    enc_layers = params['n_encoder_layers']\n",
    "    # Dynamic hidden dims\n",
    "    enc_hidden = []\n",
    "    current_dim = 256\n",
    "    for _ in range(enc_layers):\n",
    "        enc_hidden.append(current_dim)\n",
    "        current_dim = current_dim // 2\n",
    "    \n",
    "    lr_ae = params['lr_ae']\n",
    "    lr_clf = params['lr_clf']\n",
    "    dropout = params['dropout']\n",
    "    epochs_ae = 50 \n",
    "    epochs_clf = 200\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(rna_df, Y)):\n",
    "        # Data Splitting\n",
    "        tr_r, val_r = rna_df.iloc[train_idx], rna_df.iloc[val_idx]\n",
    "        tr_m, val_m = meth_df.iloc[train_idx], meth_df.iloc[val_idx]\n",
    "        tr_c, val_c = cnv_df.iloc[train_idx], cnv_df.iloc[val_idx]\n",
    "        \n",
    "        # Imputation\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        tr_r_imp = imp.fit_transform(tr_r); val_r_imp = imp.transform(val_r)\n",
    "        tr_m_imp = imp.fit_transform(tr_m); val_m_imp = imp.transform(val_m)\n",
    "        tr_c_imp = imp.fit_transform(tr_c); val_c_imp = imp.transform(val_c)\n",
    "        \n",
    "        # Variance Filter\n",
    "        def get_top_k(d, k):\n",
    "            v = np.var(d, axis=0)\n",
    "            return np.argpartition(v, -k)[-k:] if d.shape[1] > k else np.arange(d.shape[1])\n",
    "            \n",
    "        r_idx = get_top_k(tr_r_imp, n_features)\n",
    "        m_idx = get_top_k(tr_m_imp, n_features)\n",
    "        c_idx = get_top_k(tr_c_imp, n_features)\n",
    "        \n",
    "        tr_r_sel = tr_r_imp[:, r_idx]; val_r_sel = val_r_imp[:, r_idx]\n",
    "        tr_m_sel = tr_m_imp[:, m_idx]; val_m_sel = val_m_imp[:, m_idx]\n",
    "        tr_c_sel = tr_c_imp[:, c_idx]; val_c_sel = val_c_imp[:, c_idx]\n",
    "        \n",
    "        # Scaling\n",
    "        sc = StandardScaler()\n",
    "        tr_r_fin = sc.fit_transform(tr_r_sel); val_r_fin = sc.transform(val_r_sel)\n",
    "        tr_m_fin = sc.fit_transform(tr_m_sel); val_m_fin = sc.transform(val_m_sel)\n",
    "        tr_c_fin = sc.fit_transform(tr_c_sel); val_c_fin = sc.transform(val_c_sel)\n",
    "        \n",
    "        # Tensors\n",
    "        inputs_tr = {\n",
    "            'RNA': torch.FloatTensor(tr_r_fin).to(DEVICE),\n",
    "            'Meth': torch.FloatTensor(tr_m_fin).to(DEVICE),\n",
    "            'CNV': torch.FloatTensor(tr_c_fin).to(DEVICE)\n",
    "        }\n",
    "        inputs_val = {\n",
    "            'RNA': torch.FloatTensor(val_r_fin).to(DEVICE),\n",
    "            'Meth': torch.FloatTensor(val_m_fin).to(DEVICE),\n",
    "            'CNV': torch.FloatTensor(val_c_fin).to(DEVICE)\n",
    "        }\n",
    "        y_tr = torch.LongTensor(Y[train_idx]).to(DEVICE)\n",
    "        y_val = torch.LongTensor(Y[val_idx]).to(DEVICE)\n",
    "        \n",
    "        input_dims = {'RNA': tr_r_fin.shape[1], 'Meth': tr_m_fin.shape[1], 'CNV': tr_c_fin.shape[1]}\n",
    "        \n",
    "        # --- A. Train HyperDNN (Autoencoder) ---\n",
    "        model_ae = HyperDNN(\n",
    "            input_dims=input_dims,\n",
    "            latent_dim=latent_dim,\n",
    "            hyper_hidden_dim=hyper_hidden,\n",
    "            embedding_dim=embed_dim,\n",
    "            encoder_hidden_dims=enc_hidden,\n",
    "            rank=rank,\n",
    "            dropout=dropout\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        opt_ae = optim.AdamW(model_ae.parameters(), lr=lr_ae)\n",
    "        crit_mse = nn.MSELoss()\n",
    "        \n",
    "        model_ae.train()\n",
    "        for e in range(epochs_ae):\n",
    "            opt_ae.zero_grad()\n",
    "            _, recons = model_ae(inputs_tr)\n",
    "            loss = 0\n",
    "            for k in inputs_tr:\n",
    "                loss += crit_mse(recons[k], inputs_tr[k])\n",
    "            loss.backward()\n",
    "            opt_ae.step()\n",
    "            \n",
    "        # --- B. Train Classifier (on Latents) ---\n",
    "        model_ae.eval()\n",
    "        with torch.no_grad():\n",
    "            lat_tr, _ = model_ae(inputs_tr)\n",
    "            lat_val, _ = model_ae(inputs_val)\n",
    "            \n",
    "            # Fuse = Concat\n",
    "            z_tr = torch.cat([lat_tr[k] for k in ['RNA', 'Meth', 'CNV']], dim=1)\n",
    "            z_val = torch.cat([lat_val[k] for k in ['RNA', 'Meth', 'CNV']], dim=1)\n",
    "            \n",
    "        clf = SimpleClassifier(z_tr.shape[1], 128, len(np.unique(Y)), dropout).to(DEVICE)\n",
    "        opt_clf = optim.AdamW(clf.parameters(), lr=lr_clf)\n",
    "        crit_cls = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_acc = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for e in range(epochs_clf):\n",
    "            clf.train()\n",
    "            opt_clf.zero_grad()\n",
    "            out = clf(z_tr)\n",
    "            loss = crit_cls(out, y_tr)\n",
    "            loss.backward()\n",
    "            opt_clf.step()\n",
    "            \n",
    "            # Val (for early stop capability)\n",
    "            clf.eval()\n",
    "            with torch.no_grad():\n",
    "                out_v = clf(z_val)\n",
    "                acc_v = accuracy_score(y_val.cpu(), out_v.argmax(1).cpu())\n",
    "                if acc_v > best_acc:\n",
    "                    best_acc = acc_v\n",
    "                    best_state = clf.state_dict()\n",
    "        \n",
    "        if best_state: clf.load_state_dict(best_state)\n",
    "        \n",
    "        # Tests\n",
    "        clf.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = clf(z_val).argmax(1).cpu().numpy()\n",
    "            targets = y_val.cpu().numpy()\n",
    "            \n",
    "        fold_metrics['f1_macro'].append(f1_score(targets, preds, average='macro'))\n",
    "        if not is_optuna:\n",
    "            fold_metrics['f1_micro'].append(f1_score(targets, preds, average='micro'))\n",
    "            fold_metrics['accuracy'].append(accuracy_score(targets, preds))\n",
    "            fold_metrics['precision'].append(precision_score(targets, preds, average='macro', zero_division=0))\n",
    "            fold_metrics['recall'].append(recall_score(targets, preds, average='macro', zero_division=0))\n",
    "            \n",
    "    if is_optuna:\n",
    "        return np.mean(fold_metrics['f1_macro'])\n",
    "    else:\n",
    "        return {k: np.mean(v) for k, v in fold_metrics.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626a6594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> LOADING RAW ALIGNED DATA\n",
      "  Phenotype Samples (filtered): 229\n",
      "  Common Samples: 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-20 12:56:16,404] A new study created in memory with name: no-name-74f5b734-2ff9-4518-941b-a2b5b3dab59e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classes: ['Dedifferentiated liposarcoma' 'Fibromyxosarcoma' 'Leiomyosarcoma, NOS'\n",
      " 'Undifferentiated sarcoma']\n",
      "\n",
      "==================================================\n",
      "STARTING OPTUNA OPTIMIZATION FOR HYPERDNN\n",
      "==================================================\n",
      "\n",
      ">>> Feature Count: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-20 12:56:55,578] Trial 0 finished with value: 0.5044052082251792 and parameters: {'latent_dim': 64, 'hyper_hidden_dim': 64, 'embedding_dim': 16, 'rank': 8, 'n_encoder_layers': 3, 'lr_ae': 0.00015192090044952917, 'lr_clf': 0.005540929982268521, 'dropout': 0.46939870780787984}. Best is trial 0 with value: 0.5044052082251792.\n",
      "[I 2026-01-20 12:57:27,834] Trial 1 finished with value: 0.480297191616851 and parameters: {'latent_dim': 32, 'hyper_hidden_dim': 64, 'embedding_dim': 16, 'rank': 16, 'n_encoder_layers': 2, 'lr_ae': 0.00012523015749694625, 'lr_clf': 0.0002598789076802572, 'dropout': 0.4072275144776131}. Best is trial 0 with value: 0.5044052082251792.\n",
      "[I 2026-01-20 12:58:01,326] Trial 2 finished with value: 0.5157882528887721 and parameters: {'latent_dim': 32, 'hyper_hidden_dim': 32, 'embedding_dim': 32, 'rank': 16, 'n_encoder_layers': 3, 'lr_ae': 0.007950365910152305, 'lr_clf': 0.008266885139538141, 'dropout': 0.4036653596121198}. Best is trial 2 with value: 0.5157882528887721.\n",
      "[I 2026-01-20 12:58:32,733] Trial 3 finished with value: 0.5999131075613168 and parameters: {'latent_dim': 32, 'hyper_hidden_dim': 32, 'embedding_dim': 32, 'rank': 8, 'n_encoder_layers': 2, 'lr_ae': 0.00041001816183477914, 'lr_clf': 0.00039954041822725875, 'dropout': 0.21581818713206827}. Best is trial 3 with value: 0.5999131075613168.\n",
      "[I 2026-01-20 12:59:04,241] Trial 4 finished with value: 0.48305033091284805 and parameters: {'latent_dim': 32, 'hyper_hidden_dim': 32, 'embedding_dim': 16, 'rank': 16, 'n_encoder_layers': 2, 'lr_ae': 0.00013658307717993328, 'lr_clf': 0.0036267778041895143, 'dropout': 0.43413013077767315}. Best is trial 3 with value: 0.5999131075613168.\n",
      "[I 2026-01-20 12:59:37,154] Trial 5 finished with value: 0.5082980529033161 and parameters: {'latent_dim': 16, 'hyper_hidden_dim': 32, 'embedding_dim': 16, 'rank': 16, 'n_encoder_layers': 3, 'lr_ae': 0.002373938068048592, 'lr_clf': 0.008780207243239095, 'dropout': 0.3448263520271887}. Best is trial 3 with value: 0.5999131075613168.\n",
      "[I 2026-01-20 13:00:11,305] Trial 6 finished with value: 0.5664968592638872 and parameters: {'latent_dim': 32, 'hyper_hidden_dim': 32, 'embedding_dim': 16, 'rank': 8, 'n_encoder_layers': 3, 'lr_ae': 0.0028894859339706914, 'lr_clf': 0.006586628363786827, 'dropout': 0.4482461878847807}. Best is trial 3 with value: 0.5999131075613168.\n",
      "[I 2026-01-20 13:00:44,493] Trial 7 finished with value: 0.4561617520137962 and parameters: {'latent_dim': 32, 'hyper_hidden_dim': 64, 'embedding_dim': 16, 'rank': 8, 'n_encoder_layers': 3, 'lr_ae': 0.0027119924165432937, 'lr_clf': 0.0008725685899382689, 'dropout': 0.4682234431872847}. Best is trial 3 with value: 0.5999131075613168.\n",
      "[I 2026-01-20 13:01:17,200] Trial 8 finished with value: 0.625525812859909 and parameters: {'latent_dim': 32, 'hyper_hidden_dim': 64, 'embedding_dim': 16, 'rank': 16, 'n_encoder_layers': 2, 'lr_ae': 0.004192397159892664, 'lr_clf': 0.0050310173132485396, 'dropout': 0.27642202112874187}. Best is trial 8 with value: 0.625525812859909.\n",
      "[I 2026-01-20 13:01:49,828] Trial 9 finished with value: 0.5713818598008193 and parameters: {'latent_dim': 64, 'hyper_hidden_dim': 32, 'embedding_dim': 16, 'rank': 8, 'n_encoder_layers': 2, 'lr_ae': 0.008053148088543322, 'lr_clf': 0.0001827634575439328, 'dropout': 0.4717482250661004}. Best is trial 8 with value: 0.625525812859909.\n",
      "[I 2026-01-20 13:02:21,682] Trial 10 finished with value: 0.6092188348320218 and parameters: {'latent_dim': 16, 'hyper_hidden_dim': 64, 'embedding_dim': 32, 'rank': 16, 'n_encoder_layers': 2, 'lr_ae': 0.0008852221665501253, 'lr_clf': 0.002102344873173567, 'dropout': 0.25811434292500246}. Best is trial 8 with value: 0.625525812859909.\n",
      "[I 2026-01-20 13:02:54,411] Trial 11 finished with value: 0.5898564873221623 and parameters: {'latent_dim': 16, 'hyper_hidden_dim': 64, 'embedding_dim': 32, 'rank': 16, 'n_encoder_layers': 2, 'lr_ae': 0.0007818414828867214, 'lr_clf': 0.0023909430657054653, 'dropout': 0.2582580585901282}. Best is trial 8 with value: 0.625525812859909.\n",
      "[W 2026-01-20 13:03:03,916] Trial 12 failed with parameters: {'latent_dim': 16, 'hyper_hidden_dim': 64, 'embedding_dim': 32, 'rank': 16, 'n_encoder_layers': 2, 'lr_ae': 0.0009610312771507741, 'lr_clf': 0.001648693631800798, 'dropout': 0.296759666913553} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\shini\\AppData\\Local\\Temp\\ipykernel_2588\\4061720967.py\", line 29, in obj\n",
      "    return run_cv_evaluation(params, n_feat, rna_df, meth_df, cnv_df, Y, class_names, is_optuna=True)\n",
      "  File \"C:\\Users\\shini\\AppData\\Local\\Temp\\ipykernel_2588\\1368497175.py\", line 34, in run_cv_evaluation\n",
      "    tr_m_imp = imp.fit_transform(tr_m); val_m_imp = imp.transform(val_m)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\base.py\", line 894, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\impute\\_base.py\", line 625, in transform\n",
      "    X = self._validate_input(X, in_fit=False)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\impute\\_base.py\", line 360, in _validate_input\n",
      "    X = validate_data(\n",
      "        self,\n",
      "    ...<6 lines>...\n",
      "        copy=self.copy,\n",
      "    )\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2954, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 925, in check_array\n",
      "    pandas_requires_conversion = any(\n",
      "        _pandas_dtype_needs_early_conversion(i) for i in dtypes_orig\n",
      "    )\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 926, in <genexpr>\n",
      "    _pandas_dtype_needs_early_conversion(i) for i in dtypes_orig\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"f:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 710, in _pandas_dtype_needs_early_conversion\n",
      "    from pandas.api.types import is_extension_array_dtype\n",
      "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
      "KeyboardInterrupt\n",
      "[W 2026-01-20 13:03:03,995] Trial 12 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_cv_evaluation(params, n_feat, rna_df, meth_df, cnv_df, Y, class_names, is_optuna=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Best F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(param_file, \u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mobj\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj\u001b[39m(trial):\n\u001b[32m     19\u001b[39m     params = {\n\u001b[32m     20\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlatent_dim\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_categorical(\u001b[33m'\u001b[39m\u001b[33mlatent_dim\u001b[39m\u001b[33m'\u001b[39m, [\u001b[32m16\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m]),\n\u001b[32m     21\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhyper_hidden_dim\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_categorical(\u001b[33m'\u001b[39m\u001b[33mhyper_hidden_dim\u001b[39m\u001b[33m'\u001b[39m, [\u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m]),\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.5\u001b[39m)\n\u001b[32m     28\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_cv_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrna_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnv_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_optuna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrun_cv_evaluation\u001b[39m\u001b[34m(params, n_features, rna_df, meth_df, cnv_df, Y, class_names, is_optuna)\u001b[39m\n\u001b[32m     32\u001b[39m imp = SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m tr_r_imp = imp.fit_transform(tr_r); val_r_imp = imp.transform(val_r)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m tr_m_imp = \u001b[43mimp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_m\u001b[49m\u001b[43m)\u001b[49m; val_m_imp = imp.transform(val_m)\n\u001b[32m     35\u001b[39m tr_c_imp = imp.fit_transform(tr_c); val_c_imp = imp.transform(val_c)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Variance Filter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\impute\\_base.py:625\u001b[39m, in \u001b[36mSimpleImputer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[32m    611\u001b[39m \n\u001b[32m    612\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \u001b[33;03m    `X` with imputed values.\u001b[39;00m\n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    623\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m statistics = \u001b[38;5;28mself\u001b[39m.statistics_\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m1\u001b[39m] != statistics.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\impute\\_base.py:360\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    357\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py:925\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    919\u001b[39m         warnings.warn(\n\u001b[32m    920\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    921\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIt will be converted to a dense numpy array.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    922\u001b[39m         )\n\u001b[32m    924\u001b[39m dtypes_orig = \u001b[38;5;28mlist\u001b[39m(array.dtypes)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m pandas_requires_conversion = \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pandas_dtype_needs_early_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtypes_orig\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    929\u001b[39m     dtype_orig = np.result_type(*dtypes_orig)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py:926\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    919\u001b[39m         warnings.warn(\n\u001b[32m    920\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    921\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIt will be converted to a dense numpy array.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    922\u001b[39m         )\n\u001b[32m    924\u001b[39m dtypes_orig = \u001b[38;5;28mlist\u001b[39m(array.dtypes)\n\u001b[32m    925\u001b[39m pandas_requires_conversion = \u001b[38;5;28many\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[43m_pandas_dtype_needs_early_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[32m    927\u001b[39m )\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    929\u001b[39m     dtype_orig = np.result_type(*dtypes_orig)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\BRACU\\Thesis Thingy[T2510589]\\P3 Coddy Stuffs\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py:710\u001b[39m, in \u001b[36m_pandas_dtype_needs_early_conversion\u001b[39m\u001b[34m(pd_dtype)\u001b[39m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_extension_array_dtype\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists(\"../Data/expression_log.csv\"):\n",
    "        rna_df, meth_df, cnv_df, Y, class_names = load_raw_aligned_data()\n",
    "        \n",
    "        param_file = \"HyperDNN_best_params.txt\"\n",
    "        with open(param_file, 'w') as f:\n",
    "            f.write(\"Features | F1_Macro | Params\\n\")\n",
    "            \n",
    "        all_final_results = []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"STARTING OPTUNA OPTIMIZATION FOR HYPERDNN\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for n_feat in FEATURE_COUNTS:\n",
    "            print(f\"\\n>>> Feature Count: {n_feat}\")\n",
    "            \n",
    "            def obj(trial):\n",
    "                params = {\n",
    "                    'latent_dim': trial.suggest_categorical('latent_dim', [16, 32, 64]),\n",
    "                    'hyper_hidden_dim': trial.suggest_categorical('hyper_hidden_dim', [32, 64]),\n",
    "                    'embedding_dim': trial.suggest_categorical('embedding_dim', [16, 32]),\n",
    "                    'rank': trial.suggest_categorical('rank', [8, 16]),\n",
    "                    'n_encoder_layers': trial.suggest_int('n_encoder_layers', 2, 3),\n",
    "                    'lr_ae': trial.suggest_float('lr_ae', 1e-4, 1e-2, log=True),\n",
    "                    'lr_clf': trial.suggest_float('lr_clf', 1e-4, 1e-2, log=True),\n",
    "                    'dropout': trial.suggest_float('dropout', 0.2, 0.5)\n",
    "                }\n",
    "                return run_cv_evaluation(params, n_feat, rna_df, meth_df, cnv_df, Y, class_names, is_optuna=True)\n",
    "                \n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(obj, n_trials=N_TRIALS)\n",
    "            \n",
    "            print(f\"  Best F1: {study.best_value:.4f}\")\n",
    "            with open(param_file, 'a') as f:\n",
    "                f.write(f\"{n_feat} | {study.best_value:.4f} | {study.best_params}\\n\")\n",
    "                \n",
    "            # Final Eval\n",
    "            res = run_cv_evaluation(study.best_params, n_feat, rna_df, meth_df, cnv_df, Y, class_names, is_optuna=False)\n",
    "            res['n_features'] = n_feat\n",
    "            all_final_results.append(res)\n",
    "            \n",
    "        # Summary\n",
    "        df_res = pd.DataFrame(all_final_results)\n",
    "        if not df_res.empty:\n",
    "            cols = ['n_features', 'f1_macro', 'f1_micro', 'precision', 'recall', 'accuracy']\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FINAL RESULTS SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(df_res[cols].round(4).to_string(index=False))\n",
    "            df_res.to_csv(\"HyperDNN_results.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Data files not found in ../Data/ directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
