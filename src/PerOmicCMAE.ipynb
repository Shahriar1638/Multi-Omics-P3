{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46bd621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running on: cuda\n",
      ">>> Loading real data from CSV files...\n",
      "    Gene Expression: (183, 44663)\n",
      "    Methylation: (183, 396650)\n",
      "    CNV: (183, 56756)\n",
      "    Labels: (205, 1)\n",
      ">>> Common samples across all datasets: 183\n",
      ">>> NaN check - RNA: 0, Meth: 688625, CNV: 98174\n",
      ">>> Original feature dimensions: RNA=44663, Meth=396650, CNV=56756\n",
      ">>> Label classes: ['0' '2' '3']\n",
      ">>> Label distribution: {'0': np.int64(53), '2': np.int64(96), '3': np.int64(34)}\n",
      ">>> Loaded 183 samples with 3 classes (labels range: 0-2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1. INSTALL OPTUNA IF MISSING\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    print(\"Installing optuna...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION & DEVICE\n",
    "# ==========================================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\">>> Running on: {DEVICE}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Early Stopping Configuration\n",
    "EARLY_STOPPING_PATIENCE = 15  # Stop if no improvement for 15 epochs\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD REAL DATA FROM CSV FILES\n",
    "# ==========================================\n",
    "print(\">>> Loading real data from CSV files...\")\n",
    "\n",
    "# Load the three omics datasets (transposed so samples are rows)\n",
    "gene_df = pd.read_csv(\"../NewDatasets/expression_data_scaled_FXS_MOFA_3Omics.csv\", index_col=0).T\n",
    "meth_df = pd.read_csv(\"../NewDatasets/methylation_scaled_FXS_MOFA_3Omics.csv\", index_col=0).T\n",
    "cnv_df  = pd.read_csv(\"../NewDatasets/copy_number_scaled_FXS_MOFA_3Omics.csv\", index_col=0).T\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"../NewDatasets/processed_labels_3Omics_FXS_OG.csv\", index_col=0)\n",
    "\n",
    "print(f\"    Gene Expression: {gene_df.shape}\")\n",
    "print(f\"    Methylation: {meth_df.shape}\")\n",
    "print(f\"    CNV: {cnv_df.shape}\")\n",
    "print(f\"    Labels: {labels_df.shape}\")\n",
    "\n",
    "# Find common samples across all datasets\n",
    "common_samples = gene_df.index.intersection(meth_df.index).intersection(cnv_df.index).intersection(labels_df.index)\n",
    "print(f\">>> Common samples across all datasets: {len(common_samples)}\")\n",
    "\n",
    "# Align all datasets to common samples\n",
    "gene_df = gene_df.loc[common_samples]\n",
    "meth_df = meth_df.loc[common_samples]\n",
    "cnv_df = cnv_df.loc[common_samples]\n",
    "labels_df = labels_df.loc[common_samples]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_rna = gene_df.values.astype(np.float32)\n",
    "X_meth = meth_df.values.astype(np.float32)\n",
    "X_clin = cnv_df.values.astype(np.float32)  # Using CNV as the third modality\n",
    "\n",
    "# Handle NaN values in features\n",
    "print(f\">>> NaN check - RNA: {np.isnan(X_rna).sum()}, Meth: {np.isnan(X_meth).sum()}, CNV: {np.isnan(X_clin).sum()}\")\n",
    "X_rna = np.nan_to_num(X_rna, nan=0.0)\n",
    "X_meth = np.nan_to_num(X_meth, nan=0.0)\n",
    "X_clin = np.nan_to_num(X_clin, nan=0.0)\n",
    "\n",
    "# Store original (unscaled, full-feature) data for variance experiments\n",
    "X_rna_original = X_rna.copy()\n",
    "X_meth_original = X_meth.copy()\n",
    "X_clin_original = X_clin.copy()\n",
    "\n",
    "print(f\">>> Original feature dimensions: RNA={X_rna_original.shape[1]}, Meth={X_meth_original.shape[1]}, CNV={X_clin_original.shape[1]}\")\n",
    "\n",
    "# Extract labels - handle different possible column structures\n",
    "if labels_df.shape[1] == 1:\n",
    "    # Single column of labels\n",
    "    raw_labels = labels_df.iloc[:, 0].values\n",
    "else:\n",
    "    # Multiple columns - use the first one or look for common label column names\n",
    "    label_cols = [col for col in labels_df.columns if col.lower() in ['label', 'class', 'target', 'y', 'subtype']]\n",
    "    if label_cols:\n",
    "        raw_labels = labels_df[label_cols[0]].values\n",
    "    else:\n",
    "        raw_labels = labels_df.iloc[:, 0].values\n",
    "\n",
    "# Always use LabelEncoder to ensure labels are 0-indexed integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "le = LabelEncoder()\n",
    "raw_labels_clean = pd.Series(raw_labels).fillna('UNKNOWN').astype(str).values\n",
    "Y_labels = le.fit_transform(raw_labels_clean)\n",
    "print(f\">>> Label classes: {le.classes_}\")\n",
    "print(f\">>> Label distribution: {dict(zip(le.classes_, np.bincount(Y_labels)))}\")\n",
    "\n",
    "# Validate labels\n",
    "n_samples = len(Y_labels)\n",
    "n_classes = len(np.unique(Y_labels))\n",
    "assert Y_labels.min() >= 0, f\"Labels must be >= 0, got min: {Y_labels.min()}\"\n",
    "assert Y_labels.max() < n_classes, f\"Labels must be < n_classes ({n_classes}), got max: {Y_labels.max()}\"\n",
    "print(f\">>> Loaded {n_samples} samples with {n_classes} classes (labels range: {Y_labels.min()}-{Y_labels.max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de12e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Data loaded successfully! Dimensions: RNA=44663, Meth=396650, CNV=56756\n"
     ]
    }
   ],
   "source": [
    "from flexible_preprocessor import preprocess_omics\n",
    "\n",
    "\n",
    "def prepare_data_with_variance_selection(X_rna_orig, X_meth_orig, X_clin_orig, top_percent):\n",
    "    \n",
    "    \n",
    "    # Scale the selected features\n",
    "    scaler_rna = preprocess_omics(\n",
    "        X_rna_orig, \n",
    "        variance_pct=top_percent,       \n",
    "        impute_method='missforest' \n",
    "    )\n",
    "    \n",
    "    scaler_meth = preprocess_omics(\n",
    "        X_meth_orig, \n",
    "        variance_pct=top_percent,       \n",
    "        impute_method='missforest' \n",
    "    )\n",
    "    \n",
    "    scaler_clin = preprocess_omics(\n",
    "        X_clin_orig, \n",
    "        variance_pct=top_percent,       \n",
    "        impute_method='missforest' \n",
    "    )\n",
    "    \n",
    "    X_rna_scaled = scaler_rna.fit_transform(X_rna_sel)\n",
    "    X_meth_scaled = scaler_meth.fit_transform(X_meth_sel)\n",
    "    X_clin_scaled = scaler_clin.fit_transform(X_clin_sel)\n",
    "    \n",
    "    dims = (X_rna_scaled.shape[1], X_meth_scaled.shape[1], X_clin_scaled.shape[1])\n",
    "    \n",
    "    return X_rna_scaled, X_meth_scaled, X_clin_scaled, dims\n",
    "\n",
    "\n",
    "# For backward compatibility, prepare default full-feature scaled data\n",
    "scaler_rna = StandardScaler()\n",
    "scaler_meth = StandardScaler()\n",
    "scaler_clin = StandardScaler()\n",
    "\n",
    "X_rna = scaler_rna.fit_transform(X_rna_original)\n",
    "X_meth = scaler_meth.fit_transform(X_meth_original)\n",
    "X_clin = scaler_clin.fit_transform(X_clin_original)\n",
    "\n",
    "# Get dimensions for model initialization\n",
    "DIMS = (X_rna.shape[1], X_meth.shape[1], X_clin.shape[1])\n",
    "print(f\">>> Data loaded successfully! Dimensions: RNA={DIMS[0]}, Meth={DIMS[1]}, CNV={DIMS[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a6005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation loss doesn't improve.\n",
    "    Triggers when val_loss increases for 'patience' consecutive epochs (overfitting detection).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=15, min_delta=0.0, mode='min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement.\n",
    "            min_delta (float): Minimum change to qualify as an improvement.\n",
    "            mode (str): 'min' for loss (lower is better), 'max' for accuracy (higher is better).\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, score, epoch):\n",
    "        if self.mode == 'min':\n",
    "            score = -score  # Convert to maximization problem\n",
    "            \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            \n",
    "        return self.early_stop\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the early stopping state for a new fold.\"\"\"\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa2aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerOmicCMAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-Omic Contrastive Masked Autoencoder.\n",
    "    \n",
    "    Architecture optimized for high-dimensional omics data (50k-400k features) \n",
    "    with only 205 samples. Uses aggressive dimensionality reduction with \n",
    "    multiple bottleneck layers and strong regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Determine intermediate dimensions based on input size\n",
    "        # For very high-dimensional inputs, use more aggressive reduction\n",
    "        if input_dim > 200000:  # ~400k methylation features\n",
    "            hidden1 = 2048\n",
    "            hidden2 = 512\n",
    "        elif input_dim > 40000:  # ~50k/60k gene/CNV features\n",
    "            hidden1 = 1024\n",
    "            hidden2 = 256\n",
    "        else:\n",
    "            hidden1 = 512\n",
    "            hidden2 = 128\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # First layer: aggressive reduction\n",
    "            nn.Linear(input_dim, hidden1),\n",
    "            nn.LayerNorm(hidden1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # Second layer: further compression\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.LayerNorm(hidden2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # Final layer to latent space\n",
    "            nn.Linear(hidden2, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),  # Less dropout in decoder\n",
    "            nn.Linear(hidden2, hidden1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(hidden1, input_dim)\n",
    "        )\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask_ratio=0.0):\n",
    "        if mask_ratio > 0 and self.training:\n",
    "            mask = (torch.rand_like(x) > mask_ratio).float()\n",
    "            x_masked = x * mask\n",
    "        else:\n",
    "            mask = torch.ones_like(x)\n",
    "            x_masked = x\n",
    "        z = self.encoder(x_masked)\n",
    "        return self.decoder(z), self.projector(z), z, mask\n",
    "\n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Attention Fusion for multi-omics integration.\n",
    "    Enhanced with regularization for small sample sizes.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, n_classes=3, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.gate_rna = nn.Linear(latent_dim, 1)\n",
    "        self.gate_meth = nn.Linear(latent_dim, 1)\n",
    "        self.gate_clin = nn.Linear(latent_dim, 1)\n",
    "        \n",
    "        # Add hidden layer before classifier for better representation\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim // 2),\n",
    "            nn.LayerNorm(latent_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(latent_dim // 2, n_classes)\n",
    "        )\n",
    "        self.drop_rate = dropout_rate\n",
    "\n",
    "    def forward(self, z_rna, z_meth, z_clin, apply_dropout=False):\n",
    "        if apply_dropout and self.training:\n",
    "            if torch.rand(1).item() < self.drop_rate: z_rna = torch.zeros_like(z_rna)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_meth = torch.zeros_like(z_meth)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_clin = torch.zeros_like(z_clin)\n",
    "\n",
    "        w_rna = torch.sigmoid(self.gate_rna(z_rna))\n",
    "        w_meth = torch.sigmoid(self.gate_meth(z_meth))\n",
    "        w_clin = torch.sigmoid(self.gate_clin(z_clin))\n",
    "\n",
    "        z_fused = (w_rna * z_rna + w_meth * z_meth + w_clin * z_clin) / (w_rna + w_meth + w_clin + 1e-8)\n",
    "        return self.classifier(z_fused), torch.cat([w_rna, w_meth, w_clin], dim=1)\n",
    "\n",
    "\n",
    "class StabilizedUncertaintyLoss(nn.Module):\n",
    "    def __init__(self, num_losses):\n",
    "        super().__init__()\n",
    "        self.log_vars = nn.Parameter(torch.zeros(num_losses))\n",
    "    \n",
    "    def forward(self, losses):\n",
    "        total = 0\n",
    "        for i, loss in enumerate(losses):\n",
    "            prec = torch.clamp(0.5 * torch.exp(-self.log_vars[i]), 0.2, 3.0)\n",
    "            total += prec * loss + 0.5 * self.log_vars[i]\n",
    "        return total\n",
    "\n",
    "\n",
    "def contrastive_loss(q, k, queue, temp=0.1):\n",
    "    q = F.normalize(q, dim=1); k = F.normalize(k, dim=1); queue = queue.detach()\n",
    "    l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "    l_neg = torch.einsum('nc,ck->nk', [q, queue])\n",
    "    logits = torch.cat([l_pos, l_neg], dim=1) / temp\n",
    "    return F.cross_entropy(logits, torch.zeros(logits.shape[0], dtype=torch.long).to(q.device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b26ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VARIANCE-BASED FEATURE SELECTION EXPERIMENT\n",
      "================================================================================\n",
      "Testing variance thresholds: ['30%', '40%', '50%', '60%', '70%']\n",
      "Trials per experiment: 15\n",
      "Early stopping patience: 15 epochs\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      ">>> EXPERIMENT: Top 30% High-Variance Features\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Prepare data with variance selection\u001b[39;00m\n\u001b[32m     21\u001b[39m X_rna_var, X_meth_var, X_clin_var, dims_var = X_rna_original, X_meth_original, X_clin_original, var_percent\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>>> Selected features: RNA=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdims_var\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Meth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims_var[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, CNV=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims_var[\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>>> Total features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(dims_var)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (from original \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m([X_rna_original.shape[\u001b[32m1\u001b[39m],\u001b[38;5;250m \u001b[39mX_meth_original.shape[\u001b[32m1\u001b[39m],\u001b[38;5;250m \u001b[39mX_clin_original.shape[\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Create objective function for this data\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "VARIANCE_PERCENTAGES = [0.30, 0.40, 0.50, 0.60, 0.70]\n",
    "N_TRIALS_PER_EXPERIMENT = 15  # Reduced trials per experiment for faster comparison\n",
    "\n",
    "# Store results for comparison table\n",
    "experiment_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VARIANCE-BASED FEATURE SELECTION EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Testing variance thresholds: {[f'{p*100:.0f}%' for p in VARIANCE_PERCENTAGES]}\")\n",
    "print(f\"Trials per experiment: {N_TRIALS_PER_EXPERIMENT}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE} epochs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for var_percent in VARIANCE_PERCENTAGES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\">>> EXPERIMENT: Top {var_percent*100:.0f}% High-Variance Features\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare data with variance selection\n",
    "    X_rna_var, X_meth_var, X_clin_var, dims_var = X_rna_original, X_meth_original, X_clin_original, var_percent\n",
    "    \n",
    "    \n",
    "    print(f\">>> Selected features: RNA={dims_var[0]}, Meth={dims_var[1]}, CNV={dims_var[2]}\")\n",
    "    print(f\">>> Total features: {sum(dims_var)} (from original {sum([X_rna_original.shape[1], X_meth_original.shape[1], X_clin_original.shape[1]])})\")\n",
    "    \n",
    "    # Create objective function for this data\n",
    "    objective_fn = create_objective(\n",
    "        X_rna_var, X_meth_var, X_clin_var, \n",
    "        dims_var, Y_labels, n_classes, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Run Optuna optimization\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)  # Reduce Optuna verbosity\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1)\n",
    "    )\n",
    "    study.optimize(objective_fn, n_trials=N_TRIALS_PER_EXPERIMENT, show_progress_bar=False)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'variance_percent': f\"{var_percent*100:.0f}%\",\n",
    "        'rna_features': dims_var[0],\n",
    "        'meth_features': dims_var[1],\n",
    "        'cnv_features': dims_var[2],\n",
    "        'total_features': sum(dims_var),\n",
    "        'best_accuracy': study.best_value,\n",
    "        'best_params': study.best_params\n",
    "    }\n",
    "    experiment_results.append(result)\n",
    "    \n",
    "    print(f\"\\n>>> Best Accuracy for {var_percent*100:.0f}%: {study.best_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\")\n",
    "print(\"=\" * 100)\n",
    "print(\"VARIANCE EXPERIMENT RESULTS - COMPARISON TABLE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create DataFrame for results\n",
    "results_df = pd.DataFrame([{\n",
    "    'Variance %': r['variance_percent'],\n",
    "    'RNA Features': r['rna_features'],\n",
    "    'Meth Features': r['meth_features'],\n",
    "    'CNV Features': r['cnv_features'],\n",
    "    'Total Features': r['total_features'],\n",
    "    'Best Accuracy': f\"{r['best_accuracy']:.4f}\",\n",
    "    'Latent Dim': r['best_params'].get('latent_dim', 'N/A'),\n",
    "    'Dropout': f\"{r['best_params'].get('dropout_rate', 0):.3f}\",\n",
    "    'Batch Size': r['best_params'].get('batch_size', 'N/A')\n",
    "} for r in experiment_results])\n",
    "\n",
    "# Print formatted table\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Also print as markdown table for easy copy-paste\n",
    "print(\"\\n\\n### Markdown Table Format:\")\n",
    "print(results_df.to_markdown(index=False))\n",
    "\n",
    "# Find best performing variance threshold\n",
    "best_result = max(experiment_results, key=lambda x: x['best_accuracy'])\n",
    "print(f\"\\n>>> BEST PERFORMING CONFIGURATION:\")\n",
    "print(f\"    Variance Threshold: {best_result['variance_percent']}\")\n",
    "print(f\"    Best Accuracy: {best_result['best_accuracy']:.4f}\")\n",
    "print(f\"    Total Features: {best_result['total_features']}\")\n",
    "print(f\"    Best Hyperparameters:\")\n",
    "for k, v in best_result['best_params'].items():\n",
    "    print(f\"      {k}: {v}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"variance_experiment_results.csv\", index=False)\n",
    "print(f\"\\n>>> Results saved to: variance_experiment_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
