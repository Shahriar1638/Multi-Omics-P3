{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# ## Cell 2: Configuration\n",
    "\n",
    "\n",
    "RESULTS_DIR = \"Results_Mofaflex\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {os.path.abspath(RESULTS_DIR)}\")\n",
    "\n",
    "FEATURE_COUNTS = [2000, 4000, 6000]\n",
    "N_FACTORS = 15\n",
    "RANDOM_STATE = 42\n",
    "# ## Cell 3: Load Raw Data\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING RAW DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "expression_data = pd.read_csv('RawData/TCGA-SARC.star_tpm.tsv', sep='\\t', index_col=0)\n",
    "methylation_data = pd.read_csv('RawData/TCGA-SARC.methylation450.tsv', sep='\\t', index_col=0)\n",
    "copy_number_data = pd.read_csv('RawData/TCGA-SARC.gene-level_absolute.tsv', sep='\\t', index_col=0)\n",
    "\n",
    "try:\n",
    "    phenotype_data = pd.read_csv('RawData/TCGA-SARC.clinical.tsv', sep='\\t', index_col=0)\n",
    "except:\n",
    "    phenotype_data = pd.read_csv('RawData/TCGA-SARC.clinical.tsv', sep='\\t', index_col=0, on_bad_lines='skip')\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"  Expression: {expression_data.shape}\")\n",
    "print(f\"  Methylation: {methylation_data.shape}\")\n",
    "print(f\"  Copy Number: {copy_number_data.shape}\")\n",
    "print(f\"  Phenotype: {phenotype_data.shape}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE MATCHING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "samples_expression = set(expression_data.columns)\n",
    "samples_methylation = set(methylation_data.columns)\n",
    "samples_cnv = set(copy_number_data.columns)\n",
    "samples_clinical = set(phenotype_data.index)\n",
    "\n",
    "common_samples = list(samples_expression & samples_methylation & samples_cnv & samples_clinical)\n",
    "print(f\"Common samples across all omics: {len(common_samples)}\")\n",
    "\n",
    "expression_data = expression_data[common_samples]\n",
    "methylation_data = methylation_data[common_samples]\n",
    "copy_number_data = copy_number_data[common_samples]\n",
    "phenotype_data = phenotype_data.loc[common_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc678447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MINIMAL PREPROCESSING (Safe Operations)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def drop_all_nan_rows(data, name):\n",
    "    before = data.shape[0]\n",
    "    all_nan_mask = data.isna().all(axis=1)\n",
    "    data = data.loc[~all_nan_mask]\n",
    "    dropped = all_nan_mask.sum()\n",
    "    print(f\"  {name}: {before} -> {data.shape[0]} (dropped {dropped} NaN rows)\")\n",
    "    return data\n",
    "\n",
    "expression_data = drop_all_nan_rows(expression_data, \"Expression\")\n",
    "methylation_data = drop_all_nan_rows(methylation_data, \"Methylation\")\n",
    "copy_number_data = drop_all_nan_rows(copy_number_data, \"Copy Number\")\n",
    "\n",
    "# Log transformations (safe - no statistics from data)\n",
    "expression_data_log = np.log2(expression_data + 1)\n",
    "\n",
    "methylation_data = methylation_data.dropna(thresh=0.8 * methylation_data.shape[1], axis=0)\n",
    "epsilon = 1e-6\n",
    "methylation_beta = methylation_data.clip(epsilon, 1 - epsilon)\n",
    "methylation_m_values = np.log2(methylation_beta / (1 - methylation_beta))\n",
    "\n",
    "copy_number_filtered = copy_number_data.loc[copy_number_data.isnull().mean(axis=1) < 0.2]\n",
    "cnv_clipped = copy_number_filtered.clip(lower=0.05, upper=6)\n",
    "cnv_log = np.log2(cnv_clipped / 2)\n",
    "\n",
    "print(f\"\\nAfter log transforms:\")\n",
    "print(f\"  Expression: {expression_data_log.shape}\")\n",
    "print(f\"  Methylation: {methylation_m_values.shape}\")\n",
    "print(f\"  Copy Number: {cnv_log.shape}\")\n",
    "\n",
    "# Cell 6: Phenotype Processing and Label Encoding\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHENOTYPE PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "subtype_column = 'primary_diagnosis.diagnoses'\n",
    "selected_subtypes = [\n",
    "    'Leiomyosarcoma, NOS',\n",
    "    'Dedifferentiated liposarcoma',\n",
    "    'Undifferentiated sarcoma'\n",
    "]\n",
    "\n",
    "phenotype_data = phenotype_data[phenotype_data[subtype_column].isin(selected_subtypes)]\n",
    "phenotype_data_clean = phenotype_data.dropna(subset=[subtype_column])\n",
    "\n",
    "subtypes = phenotype_data_clean[subtype_column]\n",
    "label_encoder = LabelEncoder()\n",
    "subtype_encoded = pd.Series(\n",
    "    label_encoder.fit_transform(subtypes),\n",
    "    index=subtypes.index,\n",
    "    name='subtype_encoded'\n",
    ")\n",
    "\n",
    "print(\"Label encoding:\")\n",
    "for i, cls in enumerate(label_encoder.classes_):\n",
    "    count = (subtype_encoded == i).sum()\n",
    "    print(f\"  {i}: {cls} ({count} samples)\")\n",
    "\n",
    "# Update common samples\n",
    "valid_samples = list(set(common_samples) & set(phenotype_data_clean.index))\n",
    "expression_data_log = expression_data_log[valid_samples]\n",
    "methylation_m_values = methylation_m_values[valid_samples]\n",
    "cnv_log = cnv_log[valid_samples]\n",
    "subtype_encoded = subtype_encoded.loc[valid_samples]\n",
    "common_samples = valid_samples\n",
    "\n",
    "print(f\"\\nFinal sample count: {len(common_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15720389",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXTRACTING SURVIVAL DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Common survival columns in TCGA\n",
    "survival_time_cols = ['days_to_death.demographic', 'days_to_last_follow_up.diagnoses', \n",
    "                       'days_to_death', 'days_to_last_followup', 'OS.time']\n",
    "vital_status_cols = ['vital_status.demographic', 'vital_status', 'OS']\n",
    "\n",
    "# Find available columns\n",
    "time_col = None\n",
    "status_col = None\n",
    "\n",
    "for col in survival_time_cols:\n",
    "    if col in phenotype_data_clean.columns:\n",
    "        time_col = col\n",
    "        print(f\"Found survival time column: {col}\")\n",
    "        break\n",
    "\n",
    "for col in vital_status_cols:\n",
    "    if col in phenotype_data_clean.columns:\n",
    "        status_col = col\n",
    "        print(f\"Found vital status column: {col}\")\n",
    "        break\n",
    "\n",
    "if time_col and status_col:\n",
    "    survival_data = phenotype_data_clean[[time_col, status_col]].copy()\n",
    "    survival_data.columns = ['time', 'status']\n",
    "    \n",
    "    # Convert status to binary (1 = event/death, 0 = censored)\n",
    "    if survival_data['status'].dtype == object:\n",
    "        survival_data['event'] = (survival_data['status'].str.lower() == 'dead').astype(int)\n",
    "    else:\n",
    "        survival_data['event'] = survival_data['status'].astype(int)\n",
    "    \n",
    "    # Handle missing time - use days_to_last_follow_up for censored\n",
    "    if 'days_to_last_follow_up.diagnoses' in phenotype_data_clean.columns:\n",
    "        followup_col = 'days_to_last_follow_up.diagnoses'\n",
    "    elif 'days_to_last_followup' in phenotype_data_clean.columns:\n",
    "        followup_col = 'days_to_last_followup'\n",
    "    else:\n",
    "        followup_col = None\n",
    "    \n",
    "    if followup_col:\n",
    "        survival_data['time'] = survival_data['time'].fillna(\n",
    "            phenotype_data_clean[followup_col]\n",
    "        )\n",
    "    \n",
    "    # Convert to numeric and drop missing\n",
    "    survival_data['time'] = pd.to_numeric(survival_data['time'], errors='coerce')\n",
    "    survival_data = survival_data.dropna(subset=['time'])\n",
    "    survival_data = survival_data[survival_data['time'] > 0]\n",
    "    \n",
    "    # Keep only samples in common_samples\n",
    "    survival_data = survival_data.loc[survival_data.index.intersection(common_samples)]\n",
    "    \n",
    "    print(f\"\\nSurvival data available for {len(survival_data)} samples\")\n",
    "    print(f\"Events (deaths): {survival_data['event'].sum()}\")\n",
    "    print(f\"Censored: {(survival_data['event'] == 0).sum()}\")\n",
    "    print(f\"Median survival time: {survival_data['time'].median():.1f} days\")\n",
    "    \n",
    "    HAS_SURVIVAL_DATA = True\n",
    "else:\n",
    "    print(\"Survival data columns not found!\")\n",
    "    HAS_SURVIVAL_DATA = False\n",
    "    survival_data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64799cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAIN/TEST SPLIT (Before Preprocessing - No Data Leakage)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_all = subtype_encoded.loc[common_samples].values\n",
    "sample_ids = np.array(common_samples)\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(sample_ids)),\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "train_samples = sample_ids[train_indices].tolist()\n",
    "test_samples = sample_ids[test_indices].tolist()\n",
    "y_train = y_all[train_indices]\n",
    "y_test = y_all[test_indices]\n",
    "\n",
    "print(f\"Train: {len(train_samples)} samples ({len(train_samples)/len(common_samples)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_samples)} samples ({len(test_samples)/len(common_samples)*100:.1f}%)\")\n",
    "\n",
    "# Split omics data\n",
    "expr_train = expression_data_log[train_samples]\n",
    "expr_test = expression_data_log[test_samples]\n",
    "meth_train = methylation_m_values[train_samples]\n",
    "meth_test = methylation_m_values[test_samples]\n",
    "cnv_train = cnv_log[train_samples]\n",
    "cnv_test = cnv_log[test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_omics_no_leakage(train_df, test_df, name, variance_threshold=0.01):\n",
    "    \"\"\"Apply variance selection and scaling fitted ONLY on training data.\"\"\"\n",
    "    print(f\"\\n  Processing {name}...\")\n",
    "    \n",
    "    # Variance filtering (fit on train only)\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    train_T = train_df.T\n",
    "    selector.fit(train_T)\n",
    "    \n",
    "    feature_mask = selector.get_support()\n",
    "    selected_features = train_df.index[feature_mask]\n",
    "    \n",
    "    train_filtered = train_df.loc[selected_features]\n",
    "    test_filtered = test_df.loc[selected_features]\n",
    "    \n",
    "    # Scaling (fit on train only)\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(train_filtered.T).T,\n",
    "        index=train_filtered.index,\n",
    "        columns=train_filtered.columns\n",
    "    )\n",
    "    test_scaled = pd.DataFrame(\n",
    "        scaler.transform(test_filtered.T).T,\n",
    "        index=test_filtered.index,\n",
    "        columns=test_filtered.columns\n",
    "    )\n",
    "    \n",
    "    # Get feature variances for later selection\n",
    "    feature_variances = train_filtered.var(axis=1).sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"    {len(selected_features)} features retained\")\n",
    "    return train_scaled, test_scaled, feature_variances\n",
    "    \n",
    "print(\"=\" * 70)\n",
    "print(\"MAIN PREPROCESSING (Fit on Train Only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "expr_train_scaled, expr_test_scaled, expr_variances = preprocess_omics_no_leakage(\n",
    "    expr_train, expr_test, \"Expression\"\n",
    ")\n",
    "meth_train_scaled, meth_test_scaled, meth_variances = preprocess_omics_no_leakage(\n",
    "    meth_train, meth_test, \"Methylation\"\n",
    ")\n",
    "cnv_train_scaled, cnv_test_scaled, cnv_variances = preprocess_omics_no_leakage(\n",
    "    cnv_train, cnv_test, \"Copy Number\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_n_features(train_scaled, test_scaled, variances, n_features, name):\n",
    "    \"\"\"Select top N features based on variance.\"\"\"\n",
    "    if n_features >= len(variances):\n",
    "        return train_scaled, test_scaled\n",
    "    \n",
    "    top_features = variances.head(n_features).index\n",
    "    return train_scaled.loc[top_features], test_scaled.loc[top_features]\n",
    "\n",
    "def get_feature_subsets(n_features):\n",
    "    \"\"\"Get train/test data with top N features from each omics.\"\"\"\n",
    "    expr_train_top, expr_test_top = select_top_n_features(\n",
    "        expr_train_scaled, expr_test_scaled, expr_variances, n_features, \"Expression\"\n",
    "    )\n",
    "    meth_train_top, meth_test_top = select_top_n_features(\n",
    "        meth_train_scaled, meth_test_scaled, meth_variances, n_features, \"Methylation\"\n",
    "    )\n",
    "    cnv_train_top, cnv_test_top = select_top_n_features(\n",
    "        cnv_train_scaled, cnv_test_scaled, cnv_variances, n_features, \"Copy Number\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'expression': {'train': expr_train_top, 'test': expr_test_top},\n",
    "        'methylation': {'train': meth_train_top, 'test': meth_test_top},\n",
    "        'copy_number': {'train': cnv_train_top, 'test': cnv_test_top}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb060d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import mofaflex as mfl\n",
    "import mudata as md\n",
    "\n",
    "def train_mofa_model(omics_dict, n_factors=15, device_name=None):\n",
    "    \"\"\"Train MOFA model on the provided omics data.\"\"\"\n",
    "    if device_name is None:\n",
    "        gpu_idx = mfl.tl.get_free_gpu_idx()\n",
    "        device_name = f\"cuda:{gpu_idx}\" if (gpu_idx is not None and torch.cuda.is_available()) else \"cpu\"\n",
    "    \n",
    "    mdata_dict = {\n",
    "        \"copy_number\": ad.AnnData(omics_dict['copy_number']['train'].T.copy()),\n",
    "        \"methylation\": ad.AnnData(omics_dict['methylation']['train'].T.copy()),\n",
    "        \"expression\": ad.AnnData(omics_dict['expression']['train'].T.copy())\n",
    "    }\n",
    "    \n",
    "    for view_name, adata in mdata_dict.items():\n",
    "        adata.var_names = [f\"{view_name}_{name}\" for name in adata.var_names]\n",
    "    \n",
    "    mdata = md.MuData(mdata_dict)\n",
    "    \n",
    "    model = mfl.MOFAFLEX(\n",
    "        mdata,\n",
    "        mfl.ModelOptions(\n",
    "            n_factors=n_factors,\n",
    "            weight_prior=\"SpikeSlab\",\n",
    "            likelihoods=\"Normal\"\n",
    "        ),\n",
    "        mfl.TrainingOptions(\n",
    "            device=device_name,\n",
    "            seed=42,\n",
    "            lr=0.005,\n",
    "            early_stopper_patience=500,\n",
    "            save_path=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mofa_factors(model, train_samples, test_omics_dict):\n",
    "    \"\"\"Extract MOFA factors for train and test samples with NaN handling.\"\"\"\n",
    "    # Get training factors\n",
    "    factors_output = model.get_factors()\n",
    "    if isinstance(factors_output, dict):\n",
    "        group_name = list(factors_output.keys())[0]\n",
    "        factors_train = factors_output[group_name]\n",
    "    else:\n",
    "        factors_train = factors_output\n",
    "    \n",
    "    X_train = factors_train.values.copy()\n",
    "    \n",
    "    # Handle NaN in training factors\n",
    "    if np.isnan(X_train).any():\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "    \n",
    "    # Project test data\n",
    "    weights_dict = model.get_weights()\n",
    "    test_views = []\n",
    "    \n",
    "    for view_name in ['copy_number', 'methylation', 'expression']:\n",
    "        test_data = test_omics_dict[view_name]['test'].T.copy()\n",
    "        \n",
    "        # Impute NaN in test data\n",
    "        if test_data.isna().any().any():\n",
    "            test_data = test_data.fillna(test_data.median())\n",
    "            test_data = test_data.fillna(0)\n",
    "        \n",
    "        view_weights = weights_dict[view_name]\n",
    "        view_features = [f\"{view_name}_{f}\" for f in test_omics_dict[view_name]['test'].index]\n",
    "        common_features = [f for f in view_features if f in view_weights.index]\n",
    "        \n",
    "        if len(common_features) > 0:\n",
    "            W = view_weights.loc[common_features].values\n",
    "            feature_names = [f.replace(f\"{view_name}_\", \"\") for f in common_features]\n",
    "            X_test_view = test_data[feature_names].values\n",
    "            \n",
    "            if np.isnan(W).any():\n",
    "                W = np.nan_to_num(W, nan=0.0)\n",
    "            if np.isnan(X_test_view).any():\n",
    "                X_test_view = np.nan_to_num(X_test_view, nan=0.0)\n",
    "            \n",
    "            proj = X_test_view @ W\n",
    "            test_views.append(proj)\n",
    "    \n",
    "    if len(test_views) > 0:\n",
    "        stacked = np.stack(test_views, axis=0)\n",
    "        X_test = np.nanmean(stacked, axis=0)\n",
    "        if np.isnan(X_test).any():\n",
    "            X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    else:\n",
    "        X_test = np.zeros((len(test_omics_dict['expression']['test'].columns), X_train.shape[1]))\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89496ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Common.ml_classifier import train_classifiers\n",
    "def run_experiment(n_features, y_train, y_test, train_samples, test_samples):\n",
    "    \"\"\"Run complete experiment pipeline for a given number of features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"EXPERIMENT: TOP {n_features} FEATURES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get feature subsets\n",
    "    omics_data = get_feature_subsets(n_features)\n",
    "    print(f\"Expression: {omics_data['expression']['train'].shape[0]} features\")\n",
    "    print(f\"Methylation: {omics_data['methylation']['train'].shape[0]} features\")\n",
    "    print(f\"Copy Number: {omics_data['copy_number']['train'].shape[0]} features\")\n",
    "    \n",
    "    # Train MOFA\n",
    "    print(\"\\nTraining MOFA model...\")\n",
    "    model = train_mofa_model(omics_data, n_factors=N_FACTORS)\n",
    "    \n",
    "    # Get factors\n",
    "    print(\"Extracting MOFA factors...\")\n",
    "    X_train, X_test = get_mofa_factors(model, train_samples, omics_data)\n",
    "    print(f\"Train factors: {X_train.shape}, Test factors: {X_test.shape}\")\n",
    "    \n",
    "    # Save embeddings\n",
    "    train_embeddings_df = pd.DataFrame(\n",
    "        X_train, index=train_samples,\n",
    "        columns=[f'Factor_{i+1}' for i in range(X_train.shape[1])]\n",
    "    )\n",
    "    train_embeddings_df['Label'] = y_train\n",
    "    train_embeddings_df['Split'] = 'train'\n",
    "    \n",
    "    test_embeddings_df = pd.DataFrame(\n",
    "        X_test, index=test_samples,\n",
    "        columns=[f'Factor_{i+1}' for i in range(X_test.shape[1])]\n",
    "    )\n",
    "    test_embeddings_df['Label'] = y_test\n",
    "    test_embeddings_df['Split'] = 'test'\n",
    "    \n",
    "    all_embeddings_df = pd.concat([train_embeddings_df, test_embeddings_df])\n",
    "    all_embeddings_df.to_csv(f\"{RESULTS_DIR}/mofa_embeddings_{n_features}_features.csv\")\n",
    "    \n",
    "    # Train ML classifiers\n",
    "    print(\"Training ML classifiers...\")\n",
    "    ml_clf = train_classifiers(\n",
    "        X_train=X_train, X_test=X_test,\n",
    "        y_train=y_train, y_test=y_test,\n",
    "        random_state=RANDOM_STATE,\n",
    "        include_xgboost=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    results_df = ml_clf.get_results_dataframe(sort_by='Accuracy')\n",
    "    results_df['N_Features'] = n_features\n",
    "    results_df.to_csv(f\"{RESULTS_DIR}/ml_results_{n_features}_features.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nTop 5 models:\")\n",
    "    print(results_df[['Model', 'Accuracy', 'F1 (Macro)']].head(5).to_string(index=False))\n",
    "    \n",
    "    return results_df, ml_clf, model, all_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c70696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RUNNING FEATURE COUNT COMPARISON EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_results = []\n",
    "all_classifiers = {}\n",
    "all_models = {}\n",
    "all_embeddings = {}\n",
    "\n",
    "for n_features in FEATURE_COUNTS:\n",
    "    results_df, ml_clf, mofa_model, embeddings_df = run_experiment(\n",
    "        n_features, y_train, y_test, train_samples, test_samples\n",
    "    )\n",
    "    all_results.append(results_df)\n",
    "    all_classifiers[n_features] = ml_clf\n",
    "    all_models[n_features] = mofa_model\n",
    "    all_embeddings[n_features] = embeddings_df\n",
    "\n",
    "combined_results = pd.concat(all_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SIDE-BY-SIDE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_summary = []\n",
    "for n_features in FEATURE_COUNTS:\n",
    "    subset = combined_results[combined_results['N_Features'] == n_features]\n",
    "    best_row = subset.loc[subset['Accuracy'].idxmax()]\n",
    "    comparison_summary.append({\n",
    "        'N_Features': n_features,\n",
    "        'Best_Model': best_row['Model'],\n",
    "        'Accuracy': best_row['Accuracy'],\n",
    "        'F1_Macro': best_row['F1 (Macro)'],\n",
    "        'C_Index': best_row['C-Index']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_summary)\n",
    "print(\"\\nBest Model Performance by Feature Count:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Pivot tables\n",
    "pivot_accuracy = combined_results.pivot_table(\n",
    "    values='Accuracy', index='Model', columns='N_Features', aggfunc='first'\n",
    ")\n",
    "pivot_accuracy.columns = [f'{c}_features' for c in pivot_accuracy.columns]\n",
    "pivot_accuracy = pivot_accuracy.sort_values(f'{FEATURE_COUNTS[-1]}_features', ascending=False)\n",
    "\n",
    "print(\"\\nAccuracy by Model and Feature Count:\")\n",
    "print(pivot_accuracy.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Bar chart - Best accuracy\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(FEATURE_COUNTS))\n",
    "accuracies = [comparison_df.loc[comparison_df['N_Features'] == n, 'Accuracy'].values[0] for n in FEATURE_COUNTS]\n",
    "bars = ax1.bar(x_pos, accuracies, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.8)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f'{n}' for n in FEATURE_COUNTS])\n",
    "ax1.set_xlabel('Number of Top Features')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Best Model Accuracy by Feature Count')\n",
    "ax1.set_ylim([0, 1])\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{acc:.3f}', ha='center')\n",
    "\n",
    "# Line plot - Top models\n",
    "ax2 = axes[0, 1]\n",
    "for model in pivot_accuracy.index[:5]:\n",
    "    values = [pivot_accuracy.loc[model, f'{n}_features'] for n in FEATURE_COUNTS]\n",
    "    ax2.plot(FEATURE_COUNTS, values, marker='o', label=model, linewidth=2)\n",
    "ax2.set_xlabel('Number of Top Features')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Top 5 Models: Accuracy vs Feature Count')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "ax3 = axes[1, 0]\n",
    "f1_scores = [comparison_df.loc[comparison_df['N_Features'] == n, 'F1_Macro'].values[0] for n in FEATURE_COUNTS]\n",
    "bars = ax3.bar(x_pos, f1_scores, color=['#9b59b6', '#f39c12', '#1abc9c'], alpha=0.8)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f'{n}' for n in FEATURE_COUNTS])\n",
    "ax3.set_xlabel('Number of Top Features')\n",
    "ax3.set_ylabel('F1 Score (Macro)')\n",
    "ax3.set_title('Best Model F1 Score by Feature Count')\n",
    "ax3.set_ylim([0, 1])\n",
    "\n",
    "# Heatmap\n",
    "ax4 = axes[1, 1]\n",
    "heatmap_data = pivot_accuracy.iloc[:10]\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax4)\n",
    "ax4.set_title('Accuracy Heatmap (Top 10 Models)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/feature_comparison_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, n_features in enumerate(FEATURE_COUNTS):\n",
    "    ax = axes[idx]\n",
    "    embeddings = all_embeddings[n_features]\n",
    "    \n",
    "    factor_cols = [c for c in embeddings.columns if c.startswith('Factor_')]\n",
    "    X_embed = embeddings[factor_cols].values\n",
    "    labels = embeddings['Label'].values\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_embed)-1))\n",
    "    X_2d = tsne.fit_transform(X_embed)\n",
    "    \n",
    "    scatter = ax.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    ax.set_title(f't-SNE: {n_features} Features')\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter, ax=ax, label='Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/tsne_embeddings_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf21fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "    from lifelines.statistics import logrank_test, multivariate_logrank_test\n",
    "    print(\"Lifelines library loaded successfully!\")\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lifelines\"])\n",
    "    from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "    from lifelines.statistics import logrank_test, multivariate_logrank_test\n",
    "    print(\"Lifelines installed and loaded!\")\n",
    "if HAS_SURVIVAL_DATA:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PREPARING SURVIVAL DATA WITH MOFA FACTORS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use the best performing feature count\n",
    "    best_n = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'N_Features']\n",
    "    print(f\"Using embeddings from {best_n} features (best accuracy)\")\n",
    "    \n",
    "    best_embeddings = all_embeddings[best_n]\n",
    "    factor_cols = [c for c in best_embeddings.columns if c.startswith('Factor_')]\n",
    "    \n",
    "    # Merge with survival data\n",
    "    survival_factors = best_embeddings[factor_cols].copy()\n",
    "    survival_factors = survival_factors.join(survival_data[['time', 'event']], how='inner')\n",
    "    survival_factors['label'] = best_embeddings.loc[survival_factors.index, 'Label']\n",
    "    \n",
    "    # Add subtype names\n",
    "    survival_factors['subtype'] = survival_factors['label'].apply(\n",
    "        lambda x: label_encoder.classes_[int(x)]\n",
    "    )\n",
    "    \n",
    "    print(f\"Samples with survival data: {len(survival_factors)}\")\n",
    "    print(f\"Events: {survival_factors['event'].sum()}\")\n",
    "    print(f\"Censored: {(survival_factors['event'] == 0).sum()}\")\n",
    "else:\n",
    "    print(\"Survival data not available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SURVIVAL_DATA:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RISK STRATIFICATION USING MOFA FACTORS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use first few factors to create risk score\n",
    "    # Fit Cox model to find important factors\n",
    "    cox_factors = survival_factors[['time', 'event'] + factor_cols[:5]].dropna()\n",
    "    \n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(cox_factors, duration_col='time', event_col='event')\n",
    "    \n",
    "    print(\"\\nCox Proportional Hazards - Factor Importance (first 5 factors):\")\n",
    "    print(cph.summary[['coef', 'exp(coef)', 'p', 'coef lower 95%', 'coef upper 95%']])\n",
    "    \n",
    "    # Create risk score\n",
    "    risk_scores = cph.predict_partial_hazard(cox_factors)\n",
    "    median_risk = risk_scores.median()\n",
    "    \n",
    "    survival_factors_cox = survival_factors.loc[cox_factors.index].copy()\n",
    "    survival_factors_cox['risk_score'] = risk_scores\n",
    "    survival_factors_cox['risk_group'] = (risk_scores > median_risk).astype(int)\n",
    "    survival_factors_cox['risk_group'] = survival_factors_cox['risk_group'].map({0: 'Low Risk', 1: 'High Risk'})\n",
    "    \n",
    "    print(f\"\\nRisk group distribution:\")\n",
    "    print(survival_factors_cox['risk_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SURVIVAL_DATA:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    kmf = KaplanMeierFitter()\n",
    "    \n",
    "    # Low risk\n",
    "    mask_low = survival_factors_cox['risk_group'] == 'Low Risk'\n",
    "    kmf.fit(\n",
    "        survival_factors_cox.loc[mask_low, 'time'],\n",
    "        event_observed=survival_factors_cox.loc[mask_low, 'event'],\n",
    "        label='Low Risk'\n",
    "    )\n",
    "    kmf.plot_survival_function(ax=ax, color='green', linewidth=2)\n",
    "    \n",
    "    # High risk\n",
    "    mask_high = survival_factors_cox['risk_group'] == 'High Risk'\n",
    "    kmf.fit(\n",
    "        survival_factors_cox.loc[mask_high, 'time'],\n",
    "        event_observed=survival_factors_cox.loc[mask_high, 'event'],\n",
    "        label='High Risk'\n",
    "    )\n",
    "    kmf.plot_survival_function(ax=ax, color='red', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Time (days)', fontsize=12)\n",
    "    ax.set_ylabel('Survival Probability', fontsize=12)\n",
    "    ax.set_title('Kaplan-Meier Survival Curves by MOFA-based Risk Group', fontsize=14)\n",
    "    ax.legend(loc='lower left', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/kaplan_meier_by_risk_group.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Log-rank test\n",
    "    print(\"\\nLog-rank test (Low Risk vs High Risk):\")\n",
    "    results = logrank_test(\n",
    "        survival_factors_cox.loc[mask_low, 'time'],\n",
    "        survival_factors_cox.loc[mask_high, 'time'],\n",
    "        survival_factors_cox.loc[mask_low, 'event'],\n",
    "        survival_factors_cox.loc[mask_high, 'event']\n",
    "    )\n",
    "    print(f\"  Test statistic: {results.test_statistic:.4f}\")\n",
    "    print(f\"  p-value: {results.p_value:.4e}\")\n",
    "    if results.p_value < 0.05:\n",
    "        print(\"  SIGNIFICANT difference between risk groups!\")\n",
    "    else:\n",
    "        print(\"  No significant difference between risk groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SURVIVAL_DATA:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FULL COX PROPORTIONAL HAZARDS MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use all factors\n",
    "    all_factors_survival = survival_factors[['time', 'event'] + factor_cols].dropna()\n",
    "    \n",
    "    cph_full = CoxPHFitter(penalizer=0.1)  # Add regularization for stability\n",
    "    cph_full.fit(all_factors_survival, duration_col='time', event_col='event')\n",
    "    \n",
    "    print(\"\\nCox Model Summary:\")\n",
    "    print(cph_full.summary[['coef', 'exp(coef)', 'p']])\n",
    "    \n",
    "    # Concordance index\n",
    "    c_index = cph_full.concordance_index_\n",
    "    print(f\"\\nConcordance Index (C-index): {c_index:.4f}\")\n",
    "    if c_index > 0.7:\n",
    "        print(\"  Good predictive performance!\")\n",
    "    elif c_index > 0.6:\n",
    "        print(\"  Moderate predictive performance\")\n",
    "    else:\n",
    "        print(\"  Weak predictive performance\")\n",
    "    \n",
    "    # Significant factors\n",
    "    sig_factors = cph_full.summary[cph_full.summary['p'] < 0.05]\n",
    "    print(f\"\\nSignificant factors (p < 0.05): {len(sig_factors)}\")\n",
    "    if len(sig_factors) > 0:\n",
    "        print(sig_factors[['coef', 'exp(coef)', 'p']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cdf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SURVIVAL_DATA:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Hazard ratios\n",
    "    ax1 = axes[0]\n",
    "    cph_full.plot(ax=ax1)\n",
    "    ax1.set_title('Hazard Ratios (95% CI)', fontsize=12)\n",
    "    ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # C-index comparison across feature counts\n",
    "    ax2 = axes[1]\n",
    "    c_indices = []\n",
    "    for n_features in FEATURE_COUNTS:\n",
    "        emb = all_embeddings[n_features]\n",
    "        factor_cols_temp = [c for c in emb.columns if c.startswith('Factor_')]\n",
    "        \n",
    "        surv_temp = emb[factor_cols_temp].join(survival_data[['time', 'event']], how='inner').dropna()\n",
    "        if len(surv_temp) > 10:\n",
    "            try:\n",
    "                cph_temp = CoxPHFitter(penalizer=0.1)\n",
    "                cph_temp.fit(surv_temp[['time', 'event'] + factor_cols_temp], \n",
    "                            duration_col='time', event_col='event')\n",
    "                c_indices.append(cph_temp.concordance_index_)\n",
    "            except:\n",
    "                c_indices.append(np.nan)\n",
    "        else:\n",
    "            c_indices.append(np.nan)\n",
    "    \n",
    "    bars = ax2.bar(FEATURE_COUNTS, c_indices, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.8)\n",
    "    ax2.set_xlabel('Number of Features')\n",
    "    ax2.set_ylabel('C-Index')\n",
    "    ax2.set_title('Cox Model C-Index by Feature Count')\n",
    "    ax2.set_ylim([0.4, 0.9])\n",
    "    ax2.axhline(y=0.5, color='gray', linestyle='--', label='Random (0.5)')\n",
    "    ax2.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Good (0.7)')\n",
    "    for bar, ci in zip(bars, c_indices):\n",
    "        if not np.isnan(ci):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{ci:.3f}', ha='center')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/cox_model_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_SURVIVAL_DATA:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SAVING SURVIVAL ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Save survival factors with risk groups\n",
    "    survival_factors_cox.to_csv(f'{RESULTS_DIR}/survival_factors_with_risk.csv')\n",
    "    print(f\"Saved: survival_factors_with_risk.csv\")\n",
    "    \n",
    "    # Save Cox model summary\n",
    "    cox_summary = cph_full.summary.copy()\n",
    "    cox_summary.to_csv(f'{RESULTS_DIR}/cox_model_summary.csv')\n",
    "    print(f\"Saved: cox_model_summary.csv\")\n",
    "    \n",
    "    # Save survival analysis summary\n",
    "    survival_summary = pd.DataFrame({\n",
    "        'Metric': ['Total Samples', 'Events', 'Censored', 'Median Survival (days)',\n",
    "                   'C-Index', 'Significant Factors', 'Log-rank p-value (subtypes)'],\n",
    "        'Value': [\n",
    "            len(survival_factors),\n",
    "            survival_factors['event'].sum(),\n",
    "            (survival_factors['event'] == 0).sum(),\n",
    "            survival_factors['time'].median(),\n",
    "            c_index,\n",
    "            len(sig_factors) if len(sig_factors) > 0 else 0,\n",
    "            results.p_value\n",
    "        ]\n",
    "    })\n",
    "    survival_summary.to_csv(f'{RESULTS_DIR}/survival_analysis_summary.csv', index=False)\n",
    "    print(f\"Saved: survival_analysis_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAVING ALL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df.to_csv(f'{RESULTS_DIR}/feature_comparison_summary.csv', index=False)\n",
    "combined_results.to_csv(f'{RESULTS_DIR}/feature_comparison_all_results.csv', index=False)\n",
    "pivot_accuracy.to_csv(f'{RESULTS_DIR}/accuracy_by_model_features.csv')\n",
    "\n",
    "split_info = pd.DataFrame({\n",
    "    'sample_id': train_samples + test_samples,\n",
    "    'split': ['train'] * len(train_samples) + ['test'] * len(test_samples),\n",
    "    'label': list(y_train) + list(y_test)\n",
    "})\n",
    "split_info.to_csv(f'{RESULTS_DIR}/train_test_split_info.csv', index=False)\n",
    "\n",
    "label_mapping = pd.DataFrame({\n",
    "    'encoded': list(range(len(label_encoder.classes_))),\n",
    "    'subtype': label_encoder.classes_\n",
    "})\n",
    "label_mapping.to_csv(f'{RESULTS_DIR}/label_encoding_mapping.csv', index=False)\n",
    "\n",
    "print(\"\\nAll files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc3101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nResults saved to: {os.path.abspath(RESULTS_DIR)}\")\n",
    "\n",
    "print(\"\\nClassification Results:\")\n",
    "overall_best = combined_results.loc[combined_results['Accuracy'].idxmax()]\n",
    "print(f\"  Best Model: {overall_best['Model']}\")\n",
    "print(f\"  Feature Count: {overall_best['N_Features']}\")\n",
    "print(f\"  Accuracy: {overall_best['Accuracy']:.4f}\")\n",
    "print(f\"  F1 (Macro): {overall_best['F1 (Macro)']:.4f}\")\n",
    "\n",
    "if HAS_SURVIVAL_DATA:\n",
    "    print(\"\\nSurvival Analysis Results:\")\n",
    "    print(f\"  Samples analyzed: {len(survival_factors)}\")\n",
    "    print(f\"  Cox C-Index: {c_index:.4f}\")\n",
    "    print(f\"  Significant factors: {len(sig_factors) if len(sig_factors) > 0 else 0}\")\n",
    "\n",
    "print(\"\\n Pipeline Features:\")\n",
    "print(\"  - No data leakage (train/test split before preprocessing)\")\n",
    "print(\"  - No SMOTE upsampling\")\n",
    "print(f\"  - Compared {FEATURE_COUNTS} feature counts\")\n",
    "print(\"  - Survival analysis with Kaplan-Meier and Cox PH\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
