{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running on: cuda\n",
      "\n",
      "--- Running Ablation: Baseline (Gated+Focal+Noise) ---\n",
      "Result Baseline (Gated+Focal+Noise): F1-Macro = 0.6603 (+/- 0.0508)\n",
      "\n",
      "--- Running Ablation: No Gating (Concat) ---\n",
      "Result No Gating (Concat): F1-Macro = 0.6859 (+/- 0.0451)\n",
      "\n",
      "--- Running Ablation: No Noise ---\n",
      "Result No Noise: F1-Macro = 0.7114 (+/- 0.0519)\n",
      "\n",
      "--- Running Ablation: CrossEntropy Loss ---\n",
      "Result CrossEntropy Loss: F1-Macro = 0.6988 (+/- 0.0937)\n",
      "\n",
      "Ablation Results Saved.\n",
      "              Config   F1_Mean    F1_Std\n",
      "0           Baseline  0.660320  0.050796\n",
      "1          No Gating  0.685920  0.045103\n",
      "2           No Noise  0.711404  0.051918\n",
      "3  CrossEntropy Loss  0.698828  0.093710\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\">>> Running on: {DEVICE}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- BEST PARAMETERS ---\n",
    "BEST_PARAMS = {\n",
    "    'latent_dim': 24,\n",
    "    'hidden_dim': 128,\n",
    "    'dropout_encoder': 0.3284628624179063,\n",
    "    'dropout_rate': 0.35317785638083593,\n",
    "    'lr_fine': 0.0006541890601631066,\n",
    "    'weight_decay': 0.0007680199499155277,\n",
    "    'noise_level': 0.29857678286552536,\n",
    "    'focal_gamma': 4.234806377417181,\n",
    "    'alpha_scale': 1.9402234817234842\n",
    "}\n",
    "BEST_PARAMS['fusion_hidden_dim'] = BEST_PARAMS['latent_dim'] \n",
    "\n",
    "class PerOmicCMAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64, hidden_dim=256, dropout_encoder=0.0):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_encoder),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, noise_level=0.0, noise_type='gaussian', encode_only=True):\n",
    "        if self.training and noise_level > 0:\n",
    "            noise = (torch.rand_like(x) - 0.5) * 2 * noise_level # Uniform\n",
    "            x_corrupted = x + noise\n",
    "        else:\n",
    "            x_corrupted = x\n",
    "        z = self.encoder(x_corrupted)\n",
    "        return None, None, z, x_corrupted\n",
    "\n",
    "\n",
    "class GatedAttentionFusion(nn.Module):\n",
    "    def __init__(self, latent_dim=64, num_classes=4, dropout_rate=0.3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gate_rna = nn.Linear(latent_dim, 1)\n",
    "        self.gate_meth = nn.Linear(latent_dim, 1)\n",
    "        self.gate_clin = nn.Linear(latent_dim, 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        self.drop_rate = dropout_rate\n",
    "\n",
    "    def forward(self, z_rna, z_meth, z_clin, apply_dropout=False):\n",
    "        if apply_dropout and self.training:\n",
    "            if torch.rand(1).item() < self.drop_rate: z_rna = torch.zeros_like(z_rna)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_meth = torch.zeros_like(z_meth)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_clin = torch.zeros_like(z_clin)\n",
    "\n",
    "        w_rna = torch.sigmoid(self.gate_rna(z_rna))\n",
    "        w_meth = torch.sigmoid(self.gate_meth(z_meth))\n",
    "        w_clin = torch.sigmoid(self.gate_clin(z_clin))\n",
    "\n",
    "        z_fused = torch.cat([w_rna * z_rna, w_meth * z_meth, w_clin * z_clin], dim=1)\n",
    "        return self.classifier(z_fused)\n",
    "\n",
    "\n",
    "class ConcatFusion(nn.Module):\n",
    "    \"\"\"Simple Concatenation Fusion (No Gating)\"\"\"\n",
    "    def __init__(self, latent_dim=64, num_classes=4, dropout_rate=0.3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        self.drop_rate = dropout_rate\n",
    "\n",
    "    def forward(self, z_rna, z_meth, z_clin, apply_dropout=False):\n",
    "        if apply_dropout and self.training:\n",
    "            # Still keeping modality dropout for fair comparison\n",
    "            if torch.rand(1).item() < self.drop_rate: z_rna = torch.zeros_like(z_rna)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_meth = torch.zeros_like(z_meth)\n",
    "            if torch.rand(1).item() < self.drop_rate: z_clin = torch.zeros_like(z_clin)\n",
    "            \n",
    "        z_fused = torch.cat([z_rna, z_meth, z_clin], dim=1)\n",
    "        return self.classifier(z_fused)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                alpha_t = self.alpha\n",
    "            else:\n",
    "                if self.alpha.device != inputs.device:\n",
    "                    self.alpha = self.alpha.to(inputs.device)\n",
    "                alpha_t = self.alpha.gather(0, targets)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
    "\n",
    "def load_data():\n",
    "    pheno_path = \"Data/phenotype_clean.csv\"\n",
    "    if not os.path.exists(pheno_path): pheno_path = \"../Data/phenotype_clean.csv\"\n",
    "    if not os.path.exists(pheno_path): return None\n",
    "\n",
    "    pheno = pd.read_csv(pheno_path, index_col=0)\n",
    "    SUBTYPES = ['Leiomyosarcoma, NOS', 'Dedifferentiated liposarcoma',\n",
    "                'Undifferentiated sarcoma', 'Fibromyxosarcoma']\n",
    "    pheno = pheno[pheno['primary_diagnosis.diagnoses'].isin(SUBTYPES)]\n",
    "    \n",
    "    data_dir = os.path.dirname(pheno_path)\n",
    "    def load(name):\n",
    "        p = os.path.join(data_dir, name)\n",
    "        return pd.read_csv(p, index_col=0).T if os.path.exists(p) else None\n",
    "\n",
    "    rna = load(\"expression_log.csv\")\n",
    "    meth = load(\"methylation_mvalues.csv\")\n",
    "    cnv = load(\"cnv_log.csv\")\n",
    "    \n",
    "    comm = pheno.index.intersection(rna.index).intersection(meth.index).intersection(cnv.index)\n",
    "    pheno, rna, meth, cnv = pheno.loc[comm], rna.loc[comm], meth.loc[comm], cnv.loc[comm]\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(pheno['primary_diagnosis.diagnoses'])\n",
    "    \n",
    "    # Weights\n",
    "    w = len(Y) / (len(np.unique(Y)) * np.bincount(Y))\n",
    "    w = w / w.sum()\n",
    "    \n",
    "    return rna, meth, cnv, Y, le.classes_, w\n",
    "\n",
    "def prepare_fold(tr_idx, val_idx, rna, meth, cnv):\n",
    "    # Variance Filter + Impute + Scale\n",
    "    def process(df, tidx, vidx, max_f):\n",
    "        tr, val = df.values[tidx], df.values[vidx]\n",
    "        if tr.shape[1] > max_f:\n",
    "            idx = np.argpartition(np.nanvar(tr, axis=0), -max_f)[-max_f:]\n",
    "            tr, val = tr[:, idx], val[:, idx]\n",
    "        imp = KNNImputer(n_neighbors=12)\n",
    "        tr = imp.fit_transform(tr)\n",
    "        val = imp.transform(val)\n",
    "        sc = StandardScaler()\n",
    "        tr = sc.fit_transform(tr)\n",
    "        val = sc.transform(val)\n",
    "        return torch.FloatTensor(tr).to(DEVICE), torch.FloatTensor(val).to(DEVICE), tr.shape[1]\n",
    "\n",
    "    r_tr, r_v, dr = process(rna, tr_idx, val_idx, 5000)\n",
    "    m_tr, m_v, dm = process(meth, tr_idx, val_idx, 5000)\n",
    "    c_tr, c_v, dc = process(cnv, tr_idx, val_idx, 5000)\n",
    "    return r_tr, r_v, dr, m_tr, m_v, dm, c_tr, c_v, dc\n",
    "\n",
    "def run_ablation(ablation_name, rna, meth, cnv, Y, classes, weights, params):\n",
    "    print(f\"\\n--- Running Ablation: {ablation_name} ---\")\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    f1s, accs = [], []\n",
    "    \n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(rna, Y)):\n",
    "        r_tr, r_v, dr, m_tr, m_v, dm, c_tr, c_v, dc = prepare_fold(tr_idx, val_idx, rna, meth, cnv)\n",
    "        y_tr = torch.LongTensor(Y[tr_idx]).to(DEVICE)\n",
    "        y_val = torch.LongTensor(Y[val_idx]).to(DEVICE)\n",
    "        \n",
    "        # Determine Architecture\n",
    "        enc_r = PerOmicCMAE(dr, params['latent_dim'], params['hidden_dim'], params['dropout_encoder']).to(DEVICE)\n",
    "        enc_m = PerOmicCMAE(dm, params['latent_dim'], params['hidden_dim'], params['dropout_encoder']).to(DEVICE)\n",
    "        enc_c = PerOmicCMAE(dc, params['latent_dim'], params['hidden_dim'], params['dropout_encoder']).to(DEVICE)\n",
    "        \n",
    "        if ablation_name == 'No Gating (Concat)':\n",
    "            fusion = ConcatFusion(params['latent_dim'], len(classes), params['dropout_rate'], params['fusion_hidden_dim']).to(DEVICE)\n",
    "        else:\n",
    "            fusion = GatedAttentionFusion(params['latent_dim'], len(classes), params['dropout_rate'], params['fusion_hidden_dim']).to(DEVICE)\n",
    "            \n",
    "        optimizer = optim.AdamW(list(enc_r.parameters()) + list(enc_m.parameters()) + list(enc_c.parameters()) + list(fusion.parameters()),\n",
    "                               lr=params['lr_fine'], weight_decay=params['weight_decay'])\n",
    "        \n",
    "        if ablation_name == 'CrossEntropy Loss':\n",
    "             # alpha_scale not used for CE usually, but we can pass weights if we want.\n",
    "             # Strict ablation of Focal Loss meant using standard CE.\n",
    "             cw = torch.FloatTensor(weights).to(DEVICE)\n",
    "             criterion = nn.CrossEntropyLoss(weight=cw)\n",
    "        else:\n",
    "             alpha = torch.FloatTensor(weights * params['alpha_scale']).to(DEVICE)\n",
    "             criterion = FocalLoss(gamma=params['focal_gamma'], alpha=alpha)\n",
    "             \n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        best_f1 = 0\n",
    "        patience, cur_pat = 20, 0\n",
    "        warmup = 10\n",
    "        noise = 0.0 if ablation_name == 'No Noise' else params['noise_level']\n",
    "        \n",
    "        for ep in range(300): # Shorter epochs for ablation speed\n",
    "            if ep < warmup:\n",
    "                lr = params['lr_fine'] * ((ep + 1) / warmup)\n",
    "                for g in optimizer.param_groups: g['lr'] = lr\n",
    "                \n",
    "            enc_r.train(); enc_m.train(); enc_c.train(); fusion.train()\n",
    "            _, _, zr, _ = enc_r(r_tr, noise_level=noise, noise_type='uniform')\n",
    "            _, _, zm, _ = enc_m(m_tr, noise_level=noise, noise_type='uniform')\n",
    "            _, _, zc, _ = enc_c(c_tr, noise_level=noise, noise_type='uniform')\n",
    "            \n",
    "            logits = fusion(zr, zm, zc, apply_dropout=True)\n",
    "            loss = criterion(logits, y_tr)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            enc_r.eval(); enc_m.eval(); enc_c.eval(); fusion.eval()\n",
    "            with torch.no_grad():\n",
    "                _, _, zrv, _ = enc_r(r_v); _, _, zmv, _ = enc_m(m_v); _, _, zcv, _ = enc_c(c_v)\n",
    "                lv = fusion(zrv, zmv, zcv)\n",
    "                pred = lv.argmax(1).cpu().numpy()\n",
    "                cur_f1 = f1_score(y_val.cpu().numpy(), pred, average='macro')\n",
    "                \n",
    "            if ep > warmup: scheduler.step(cur_f1)\n",
    "            \n",
    "            if cur_f1 > best_f1:\n",
    "                best_f1 = cur_f1\n",
    "                cur_pat = 0\n",
    "            else:\n",
    "                cur_pat += 1\n",
    "                if cur_pat >= patience and ep >= warmup: break\n",
    "        \n",
    "        f1s.append(best_f1)\n",
    "        \n",
    "    mean, std = np.mean(f1s), np.std(f1s)\n",
    "    print(f\"Result {ablation_name}: F1-Macro = {mean:.4f} (+/- {std:.4f})\")\n",
    "    return mean, std\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if load_data() is None:\n",
    "        print(\"Data not found.\")\n",
    "        sys.exit()\n",
    "    rna, meth, cnv, Y, classes, w = load_data()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 1. Baseline\n",
    "    m, s = run_ablation('Baseline (Gated+Focal+Noise)', rna, meth, cnv, Y, classes, w, BEST_PARAMS)\n",
    "    results.append({'Config': 'Baseline', 'F1_Mean': m, 'F1_Std': s})\n",
    "    \n",
    "    # 2. No Gating\n",
    "    m, s = run_ablation('No Gating (Concat)', rna, meth, cnv, Y, classes, w, BEST_PARAMS)\n",
    "    results.append({'Config': 'No Gating', 'F1_Mean': m, 'F1_Std': s})\n",
    "    \n",
    "    # 3. No Noise\n",
    "    m, s = run_ablation('No Noise', rna, meth, cnv, Y, classes, w, BEST_PARAMS)\n",
    "    results.append({'Config': 'No Noise', 'F1_Mean': m, 'F1_Std': s})\n",
    "    \n",
    "    # 4. Cross Entropy\n",
    "    m, s = run_ablation('CrossEntropy Loss', rna, meth, cnv, Y, classes, w, BEST_PARAMS)\n",
    "    results.append({'Config': 'CrossEntropy Loss', 'F1_Mean': m, 'F1_Std': s})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"Ablation_Results_Architecture.csv\", index=False)\n",
    "    print(\"\\nAblation Results Saved.\")\n",
    "    print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
